01/29/2025 03:32:32 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 98790.23 examples/s]
01/29/2025 03:32:32 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.94s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  3.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.87s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 44.85 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:02, 121.75 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 170.71 examples/s]Preparing prompts (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [00:00<00:00, 205.57 examples/s]Preparing prompts (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 250.05 examples/s]Preparing prompts (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 216.66 examples/s]Preparing prompts (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 219/300 [00:01<00:00, 257.90 examples/s]Preparing prompts (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [00:01<00:00, 255.79 examples/s]Preparing prompts (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 291/300 [00:01<00:00, 279.09 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 219.25 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:15<02:22, 15.86s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:58, 14.76s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:44, 14.94s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:27, 14.62s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:14, 14.82s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:30<01:00, 15.24s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:43<00:44, 14.69s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:00<00:30, 15.28s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:16<00:15, 15.63s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:23<00:00, 12.88s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 48.15 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 148.75 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 187.60 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 212.01 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 234.99 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 215.49 examples/s][A
Postprocessing dataset (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [00:00<00:00, 243.33 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 233.13 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 241.34 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 269.40 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 214.08 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 36684.87 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 35993.34 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37523.97 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 36812.59 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:25<00:00, 14.57s/it]
01/29/2025 03:35:13 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 299 examples [00:00, 103473.34 examples/s]
01/29/2025 03:35:13 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.83s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.63s/it]
Preparing prompts (num_proc=32):   0%|          | 0/299 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/299 [00:00<00:06, 44.37 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 40/299 [00:00<00:01, 142.14 examples/s]Preparing prompts (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/299 [00:00<00:01, 212.56 examples/s]Preparing prompts (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/299 [00:00<00:00, 220.39 examples/s]Preparing prompts (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 137/299 [00:00<00:00, 209.23 examples/s]Preparing prompts (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 173/299 [00:00<00:00, 242.95 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 200/299 [00:00<00:00, 238.58 examples/s]Preparing prompts (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 227/299 [00:01<00:00, 218.88 examples/s]Preparing prompts (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 263/299 [00:01<00:00, 239.58 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 218.56 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:14<02:12, 14.78s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:47, 13.47s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:38<01:25, 12.26s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:50<01:14, 12.44s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:04<01:03, 12.74s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:16<00:49, 12.48s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:29<00:38, 12.74s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:40<00:24, 12.32s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:53<00:12, 12.48s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:59<00:00, 10.48s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/299 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/299 [00:00<00:05, 50.67 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/299 [00:00<00:02, 114.82 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/299 [00:00<00:01, 184.51 examples/s][A
Postprocessing dataset (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 128/299 [00:00<00:00, 242.29 examples/s][A
Postprocessing dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 164/299 [00:00<00:00, 232.57 examples/s][A
Postprocessing dataset (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 191/299 [00:00<00:00, 235.42 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 227/299 [00:01<00:00, 230.24 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 263/299 [00:01<00:00, 245.72 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 264.00 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 212.78 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/299 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:00<00:00, 37697.93 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:00<00:00, 37107.85 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/299 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:00<00:00, 37821.85 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:00<00:00, 37184.87 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:01<00:00, 12.19s/it]
01/29/2025 03:37:35 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 180 examples [00:00, 24294.46 examples/s]
01/29/2025 03:37:35 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.75s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.52s/it]
Preparing prompts (num_proc=32):   0%|          | 0/180 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 6/180 [00:00<00:05, 29.56 examples/s]Preparing prompts (num_proc=32):  17%|â–ˆâ–‹        | 30/180 [00:00<00:01, 105.81 examples/s]Preparing prompts (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 48/180 [00:00<00:01, 110.94 examples/s]Preparing prompts (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 66/180 [00:00<00:00, 125.42 examples/s]Preparing prompts (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 84/180 [00:00<00:00, 138.95 examples/s]Preparing prompts (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 102/180 [00:00<00:00, 138.70 examples/s]Preparing prompts (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 125/180 [00:00<00:00, 154.15 examples/s]Preparing prompts (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 145/180 [00:01<00:00, 153.83 examples/s]Preparing prompts (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 165/180 [00:01<00:00, 151.29 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:01<00:00, 133.83 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/6 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  17%|â–ˆâ–‹        | 1/6 [00:14<01:13, 14.62s/it] ... :  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:28<00:55, 13.91s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:40<00:40, 13.45s/it] ... :  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:54<00:26, 13.36s/it] ... :  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [01:07<00:13, 13.41s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:18<00:00, 12.62s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/180 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 6/180 [00:00<00:05, 32.21 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 30/180 [00:00<00:01, 117.36 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 48/180 [00:00<00:01, 124.61 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 66/180 [00:00<00:00, 137.14 examples/s][A
Postprocessing dataset (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 84/180 [00:00<00:00, 133.25 examples/s][A
Postprocessing dataset (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 114/180 [00:00<00:00, 164.73 examples/s][A
Postprocessing dataset (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 135/180 [00:00<00:00, 172.73 examples/s][A
Postprocessing dataset (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 155/180 [00:01<00:00, 159.78 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 175/180 [00:01<00:00, 162.40 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:01<00:00, 138.85 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/180 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 19028.02 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 18698.60 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/180 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 19675.66 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 19259.56 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:20<00:00, 13.46s/it]
01/29/2025 03:39:13 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 44582.31 examples/s]
01/29/2025 03:39:14 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.68s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.58s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.47s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.51s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 49.65 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 134.53 examples/s]Preparing prompts (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 206.79 examples/s]Preparing prompts (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 206.38 examples/s]Preparing prompts (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 249.21 examples/s]Preparing prompts (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 238.49 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 243.68 examples/s]Preparing prompts (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 246.18 examples/s]Preparing prompts (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [00:01<00:00, 227.70 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 279.98 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 219.82 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:15<02:16, 15.11s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:52, 14.05s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.51s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:56<01:22, 13.80s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:09<01:08, 13.78s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:25<00:57, 14.26s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:38<00:41, 13.95s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:54<00:29, 14.54s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:07<00:14, 14.29s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:14<00:00, 12.02s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 48.80 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 157.84 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 193.70 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 222.63 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 226.40 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 225.24 examples/s][A
Postprocessing dataset (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [00:00<00:00, 250.16 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 240.46 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 222.39 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 255.90 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 211.53 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37300.39 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 36531.51 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 38098.86 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37415.74 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:17<00:00, 13.71s/it]
01/29/2025 03:41:48 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 214 examples [00:00, 50479.78 examples/s]
01/29/2025 03:41:48 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.71s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.51s/it]
Preparing prompts (num_proc=32):   0%|          | 0/214 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 7/214 [00:00<00:05, 35.96 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 28/214 [00:00<00:02, 86.38 examples/s]Preparing prompts (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 63/214 [00:00<00:01, 134.20 examples/s]Preparing prompts (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 91/214 [00:00<00:00, 145.55 examples/s]Preparing prompts (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 112/214 [00:00<00:00, 147.61 examples/s]Preparing prompts (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 133/214 [00:00<00:00, 155.26 examples/s]Preparing prompts (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 160/214 [00:01<00:00, 174.59 examples/s]Preparing prompts (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 190/214 [00:01<00:00, 203.36 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214/214 [00:01<00:00, 157.34 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/7 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  14%|â–ˆâ–        | 1/7 [00:16<01:41, 16.84s/it] ... :  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:27<01:07, 13.47s/it] ... :  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:39<00:50, 12.54s/it] ... :  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:50<00:36, 12.15s/it] ... :  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [01:03<00:24, 12.25s/it] ... :  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [01:15<00:12, 12.36s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:23<00:00, 10.81s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/214 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 7/214 [00:00<00:05, 36.76 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–‹        | 35/214 [00:00<00:01, 130.45 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 56/214 [00:00<00:01, 155.45 examples/s][A
Postprocessing dataset (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 77/214 [00:00<00:00, 149.08 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 105/214 [00:00<00:00, 166.41 examples/s][A
Postprocessing dataset (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 126/214 [00:00<00:00, 176.73 examples/s][A
Postprocessing dataset (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 154/214 [00:00<00:00, 191.77 examples/s][A
Postprocessing dataset (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 178/214 [00:01<00:00, 166.81 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 202/214 [00:01<00:00, 182.72 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214/214 [00:01<00:00, 159.89 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/214 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214/214 [00:00<00:00, 27553.45 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214/214 [00:00<00:00, 27113.16 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/214 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214/214 [00:00<00:00, 27016.04 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214/214 [00:00<00:00, 26616.29 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:25<00:00, 12.24s/it]
01/29/2025 03:43:32 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 243 examples [00:00, 35069.19 examples/s]
01/29/2025 03:43:32 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.68s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.50s/it]
Preparing prompts (num_proc=32):   0%|          | 0/243 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/243 [00:00<00:06, 36.00 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 32/243 [00:00<00:01, 113.10 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 56/243 [00:00<00:01, 150.55 examples/s]Preparing prompts (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/243 [00:00<00:00, 188.60 examples/s]Preparing prompts (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 112/243 [00:00<00:00, 181.60 examples/s]Preparing prompts (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 136/243 [00:00<00:00, 177.42 examples/s]Preparing prompts (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 173/243 [00:01<00:00, 192.24 examples/s]Preparing prompts (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 194/243 [00:01<00:00, 180.48 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [00:01<00:00, 246.09 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [00:01<00:00, 180.93 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:15<01:50, 15.78s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:30<01:32, 15.36s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:45<01:15, 15.01s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [01:02<01:03, 15.83s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:16<00:45, 15.19s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:31<00:29, 14.94s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:45<00:14, 14.69s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:54<00:00, 12.84s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/243 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/243 [00:00<00:05, 42.38 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–‹        | 40/243 [00:00<00:01, 148.68 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–‹       | 64/243 [00:00<00:01, 156.77 examples/s][A
Postprocessing dataset (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/243 [00:00<00:00, 170.82 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 120/243 [00:00<00:00, 212.41 examples/s][A
Postprocessing dataset (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 144/243 [00:00<00:00, 194.15 examples/s][A
Postprocessing dataset (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 166/243 [00:00<00:00, 197.30 examples/s][A
Postprocessing dataset (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 187/243 [00:01<00:00, 197.20 examples/s][A
Postprocessing dataset (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 208/243 [00:01<00:00, 186.65 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [00:01<00:00, 229.88 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [00:01<00:00, 181.72 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/243 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [00:00<00:00, 30715.93 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [00:00<00:00, 30068.91 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/243 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [00:00<00:00, 29462.22 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [00:00<00:00, 29031.70 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:56<00:00, 14.52s/it]
01/29/2025 03:45:46 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 200 examples [00:00, 49994.68 examples/s]
01/29/2025 03:45:46 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.46s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.36s/it]
Preparing prompts (num_proc=32):   0%|          | 0/200 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   4%|â–Ž         | 7/200 [00:00<00:05, 36.57 examples/s]Preparing prompts (num_proc=32):  10%|â–ˆ         | 21/200 [00:00<00:02, 78.95 examples/s]Preparing prompts (num_proc=32):  21%|â–ˆâ–ˆ        | 42/200 [00:00<00:01, 105.87 examples/s]Preparing prompts (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [00:00<00:01, 119.78 examples/s]Preparing prompts (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [00:00<00:00, 155.26 examples/s]Preparing prompts (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [00:00<00:00, 168.25 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [00:01<00:00, 150.39 examples/s]Preparing prompts (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [00:01<00:00, 174.33 examples/s]Preparing prompts (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [00:01<00:00, 188.85 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:01<00:00, 148.09 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/7 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  14%|â–ˆâ–        | 1/7 [00:11<01:11, 11.85s/it] ... :  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:23<00:59, 11.84s/it] ... :  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:35<00:47, 11.77s/it] ... :  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:45<00:33, 11.32s/it] ... :  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:58<00:23, 11.69s/it] ... :  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [01:08<00:11, 11.30s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:13<00:00,  9.19s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/200 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   4%|â–Ž         | 7/200 [00:00<00:05, 35.87 examples/s][A
Postprocessing dataset (num_proc=32):  21%|â–ˆâ–ˆ        | 42/200 [00:00<00:01, 117.36 examples/s][A
Postprocessing dataset (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [00:00<00:00, 136.42 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:00<00:00, 177.58 examples/s][A
Postprocessing dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [00:00<00:00, 162.92 examples/s][A
Postprocessing dataset (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [00:00<00:00, 160.36 examples/s][A
Postprocessing dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [00:01<00:00, 162.82 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [00:01<00:00, 179.32 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:01<00:00, 152.51 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 24592.81 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 24211.64 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 25455.51 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 25006.28 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:15<00:00, 10.82s/it]
01/29/2025 03:47:19 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 184 examples [00:00, 36056.44 examples/s]
01/29/2025 03:47:19 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.38s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.37s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.31s/it]
Preparing prompts (num_proc=32):   0%|          | 0/184 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 6/184 [00:00<00:06, 29.16 examples/s]Preparing prompts (num_proc=32):  16%|â–ˆâ–‹        | 30/184 [00:00<00:01, 106.79 examples/s]Preparing prompts (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 48/184 [00:00<00:01, 123.94 examples/s]Preparing prompts (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 66/184 [00:00<00:00, 139.44 examples/s]Preparing prompts (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 84/184 [00:00<00:00, 120.42 examples/s]Preparing prompts (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 102/184 [00:00<00:00, 126.35 examples/s]Preparing prompts (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 126/184 [00:00<00:00, 145.93 examples/s]Preparing prompts (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 149/184 [00:01<00:00, 162.20 examples/s]Preparing prompts (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 169/184 [00:01<00:00, 164.77 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:01<00:00, 134.53 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/6 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  17%|â–ˆâ–‹        | 1/6 [00:11<00:56, 11.37s/it] ... :  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:23<00:47, 11.78s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:34<00:34, 11.41s/it] ... :  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:45<00:22, 11.18s/it] ... :  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:57<00:11, 11.50s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:08<00:00, 11.41s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/184 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 6/184 [00:00<00:06, 25.72 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–‹        | 30/184 [00:00<00:01, 93.96 examples/s][A
Postprocessing dataset (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 54/184 [00:00<00:01, 129.22 examples/s][A
Postprocessing dataset (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 72/184 [00:00<00:00, 132.80 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 90/184 [00:00<00:00, 140.75 examples/s][A
Postprocessing dataset (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 114/184 [00:00<00:00, 153.46 examples/s][A
Postprocessing dataset (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 138/184 [00:01<00:00, 157.86 examples/s][A
Postprocessing dataset (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 159/184 [00:01<00:00, 164.14 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 179/184 [00:01<00:00, 167.99 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:01<00:00, 137.06 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/184 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:00<00:00, 23184.09 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:00<00:00, 22791.09 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/184 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:00<00:00, 23710.47 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:00<00:00, 23342.57 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:10<00:00, 11.77s/it]
01/29/2025 03:48:47 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 65386.16 examples/s]
01/29/2025 03:48:47 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.33s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.55s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.48s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 50.95 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 138.61 examples/s]Preparing prompts (num_proc=32):  20%|â–ˆâ–ˆ        | 60/300 [00:00<00:01, 157.31 examples/s]Preparing prompts (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [00:00<00:00, 224.54 examples/s]Preparing prompts (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 129/300 [00:00<00:00, 232.77 examples/s]Preparing prompts (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 156/300 [00:00<00:00, 229.51 examples/s]Preparing prompts (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 183/300 [00:00<00:00, 237.42 examples/s]Preparing prompts (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [00:01<00:00, 234.70 examples/s]Preparing prompts (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 246/300 [00:01<00:00, 250.06 examples/s]Preparing prompts (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [00:01<00:00, 269.19 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 219.49 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:14<02:13, 14.82s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:49, 13.74s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:38, 14.04s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:54<01:20, 13.37s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:06<01:03, 12.75s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:20<00:53, 13.36s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:34<00:40, 13.55s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:51<00:29, 14.66s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:04<00:14, 14.12s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:11<00:00, 11.87s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 49.83 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 140.71 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 214.56 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 201.78 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 240.54 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 231.55 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 235.72 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 232.94 examples/s][A
Postprocessing dataset (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [00:01<00:00, 230.70 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [00:01<00:00, 240.46 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 210.86 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37893.49 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37238.57 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 38398.83 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37663.24 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:13<00:00, 13.37s/it]
01/29/2025 03:51:19 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 250 examples [00:00, 46636.54 examples/s]
01/29/2025 03:51:19 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.74s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.48s/it]
Preparing prompts (num_proc=32):   0%|          | 0/250 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/250 [00:00<00:06, 36.73 examples/s]Preparing prompts (num_proc=32):  10%|â–‰         | 24/250 [00:00<00:02, 83.11 examples/s]Preparing prompts (num_proc=32):  22%|â–ˆâ–ˆâ–       | 56/250 [00:00<00:01, 149.54 examples/s]Preparing prompts (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 80/250 [00:00<00:01, 163.29 examples/s]Preparing prompts (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 104/250 [00:00<00:00, 164.11 examples/s]Preparing prompts (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 128/250 [00:00<00:00, 175.26 examples/s]Preparing prompts (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 152/250 [00:00<00:00, 187.54 examples/s]Preparing prompts (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 192/250 [00:01<00:00, 227.48 examples/s]Preparing prompts (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 222/250 [00:01<00:00, 235.29 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 187.55 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:14<01:42, 14.62s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:31<01:35, 15.90s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:50<01:26, 17.25s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [01:05<01:05, 16.32s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:20<00:48, 16.13s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:35<00:31, 15.57s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:48<00:14, 14.79s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:01<00:00, 14.07s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/250 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/250 [00:00<00:06, 39.00 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 32/250 [00:00<00:02, 104.90 examples/s][A
Postprocessing dataset (num_proc=32):  19%|â–ˆâ–‰        | 48/250 [00:00<00:01, 116.76 examples/s][A
Postprocessing dataset (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 72/250 [00:00<00:01, 140.60 examples/s][A
Postprocessing dataset (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 104/250 [00:00<00:00, 188.18 examples/s][A
Postprocessing dataset (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 128/250 [00:00<00:00, 193.16 examples/s][A
Postprocessing dataset (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 160/250 [00:00<00:00, 188.52 examples/s][A
Postprocessing dataset (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 192/250 [00:01<00:00, 205.15 examples/s][A
Postprocessing dataset (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 222/250 [00:01<00:00, 212.73 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 176.95 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/250 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 31356.94 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 30793.37 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/250 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 31987.31 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 31455.71 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:03<00:00, 15.41s/it]
01/29/2025 03:53:41 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 40402.36 examples/s]
01/29/2025 03:53:41 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.79s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.58s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.47s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.52s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 42.82 examples/s]Preparing prompts (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 162.52 examples/s]Preparing prompts (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 204.91 examples/s]Preparing prompts (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 198.19 examples/s]Preparing prompts (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 226.07 examples/s]Preparing prompts (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 211.82 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:01<00:00, 220.58 examples/s]Preparing prompts (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 246/300 [00:01<00:00, 264.81 examples/s]Preparing prompts (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [00:01<00:00, 277.67 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 223.63 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:15<02:19, 15.54s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:30<01:59, 14.99s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:47<01:51, 15.95s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:02<01:32, 15.48s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:15<01:13, 14.69s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:30<00:58, 14.71s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:43<00:42, 14.25s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:57<00:28, 14.09s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:12<00:14, 14.62s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:20<00:00, 12.41s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:08, 34.42 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 136.17 examples/s][A
Postprocessing dataset (num_proc=32):  30%|â–ˆâ–ˆâ–ˆ       | 90/300 [00:00<00:01, 170.43 examples/s][A
Postprocessing dataset (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 120/300 [00:00<00:00, 196.10 examples/s][A
Postprocessing dataset (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 156/300 [00:00<00:00, 226.76 examples/s][A
Postprocessing dataset (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 192/300 [00:00<00:00, 250.16 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 224.15 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 241.75 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 258.79 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 203.03 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 38375.41 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37709.52 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 38913.01 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 38252.91 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:22<00:00, 14.26s/it]
01/29/2025 03:56:21 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 249 examples [00:00, 84510.58 examples/s]
01/29/2025 03:56:21 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.67s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.51s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.55s/it]
Preparing prompts (num_proc=32):   0%|          | 0/249 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/249 [00:00<00:06, 36.29 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 32/249 [00:00<00:02, 103.92 examples/s]Preparing prompts (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 72/249 [00:00<00:00, 178.61 examples/s]Preparing prompts (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 96/249 [00:00<00:00, 192.20 examples/s]Preparing prompts (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 120/249 [00:00<00:00, 181.77 examples/s]Preparing prompts (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 152/249 [00:00<00:00, 179.07 examples/s]Preparing prompts (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 184/249 [00:01<00:00, 209.46 examples/s]Preparing prompts (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 207/249 [00:01<00:00, 196.00 examples/s]Preparing prompts (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 242/249 [00:01<00:00, 226.39 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:01<00:00, 182.35 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:14<01:43, 14.75s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:24<01:12, 12.07s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:36<00:59, 11.81s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:47<00:46, 11.51s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:00<00:36, 12.13s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:15<00:26, 13.20s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:30<00:13, 13.59s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:40<00:00, 12.38s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/249 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/249 [00:00<00:06, 39.40 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–Œ        | 40/249 [00:00<00:01, 131.76 examples/s][A
Postprocessing dataset (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 72/249 [00:00<00:00, 179.02 examples/s][A
Postprocessing dataset (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 96/249 [00:00<00:00, 189.96 examples/s][A
Postprocessing dataset (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 120/249 [00:00<00:00, 181.50 examples/s][A
Postprocessing dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 152/249 [00:00<00:00, 204.12 examples/s][A
Postprocessing dataset (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 176/249 [00:00<00:00, 194.42 examples/s][A
Postprocessing dataset (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 207/249 [00:01<00:00, 204.71 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 235/249 [00:01<00:00, 219.29 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:01<00:00, 183.80 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/249 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:00<00:00, 31210.04 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:00<00:00, 30692.74 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/249 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:00<00:00, 31776.00 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:00<00:00, 31165.34 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:42<00:00, 12.79s/it]
01/29/2025 03:58:21 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 58071.40 examples/s]
01/29/2025 03:58:21 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.48s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.37s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.39s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 56.44 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 145.45 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 175.11 examples/s]Preparing prompts (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 230.86 examples/s]Preparing prompts (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 176.50 examples/s]Preparing prompts (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 165/300 [00:00<00:00, 190.34 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:01<00:00, 226.44 examples/s]Preparing prompts (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 233.29 examples/s]Preparing prompts (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [00:01<00:00, 292.97 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 219.43 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:12<01:49, 12.16s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:23<01:32, 11.60s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:35<01:21, 11.62s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:46<01:09, 11.56s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:57<00:56, 11.23s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:09<00:45, 11.47s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:22<00:36, 12.09s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:35<00:24, 12.27s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:51<00:13, 13.43s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:57<00:00, 11.25s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 42.67 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:02, 126.94 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 212.08 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 211.85 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 256.80 examples/s][A
Postprocessing dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 183/300 [00:00<00:00, 256.21 examples/s][A
Postprocessing dataset (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 219/300 [00:01<00:00, 222.26 examples/s][A
Postprocessing dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 246/300 [00:01<00:00, 220.77 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [00:01<00:00, 249.98 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 213.45 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37891.21 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37322.51 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37463.64 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 36869.76 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:59<00:00, 11.96s/it]
01/29/2025 04:00:39 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 275 examples [00:00, 38577.66 examples/s]
01/29/2025 04:00:39 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.72s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.55s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.52s/it]
Preparing prompts (num_proc=32):   0%|          | 0/275 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 9/275 [00:00<00:05, 48.22 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 36/275 [00:00<00:01, 138.32 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 63/275 [00:00<00:01, 172.93 examples/s]Preparing prompts (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/275 [00:00<00:01, 151.83 examples/s]Preparing prompts (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 126/275 [00:00<00:00, 194.42 examples/s]Preparing prompts (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 171/275 [00:00<00:00, 217.47 examples/s]Preparing prompts (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 195/275 [00:01<00:00, 221.93 examples/s]Preparing prompts (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 235/275 [00:01<00:00, 255.44 examples/s]Preparing prompts (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/275 [00:01<00:00, 263.23 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275/275 [00:01<00:00, 205.74 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/9 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  11%|â–ˆ         | 1/9 [00:12<01:41, 12.74s/it] ... :  22%|â–ˆâ–ˆâ–       | 2/9 [00:24<01:25, 12.26s/it] ... :  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:37<01:14, 12.43s/it] ... :  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:51<01:05, 13.09s/it] ... :  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [01:08<00:58, 14.56s/it] ... :  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [01:20<00:41, 13.71s/it] ... :  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [01:36<00:28, 14.41s/it] ... :  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:48<00:13, 13.53s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:56<00:00, 11.83s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/275 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 9/275 [00:00<00:06, 39.37 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–‹        | 45/275 [00:00<00:01, 134.40 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 72/275 [00:00<00:01, 168.81 examples/s][A
Postprocessing dataset (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 99/275 [00:00<00:00, 195.55 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 126/275 [00:00<00:00, 194.73 examples/s][A
Postprocessing dataset (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 162/275 [00:00<00:00, 214.53 examples/s][A
Postprocessing dataset (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 187/275 [00:00<00:00, 218.62 examples/s][A
Postprocessing dataset (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 211/275 [00:01<00:00, 216.85 examples/s][A
Postprocessing dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 251/275 [00:01<00:00, 243.00 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275/275 [00:01<00:00, 198.90 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/275 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275/275 [00:00<00:00, 33997.51 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275/275 [00:00<00:00, 33423.17 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/275 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275/275 [00:00<00:00, 34705.39 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275/275 [00:00<00:00, 34091.97 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:58<00:00, 13.14s/it]
01/29/2025 04:02:56 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 225 examples [00:00, 31315.32 examples/s]
01/29/2025 04:02:56 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.72s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.38s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.45s/it]
Preparing prompts (num_proc=32):   0%|          | 0/225 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   4%|â–Ž         | 8/225 [00:00<00:06, 31.07 examples/s]Preparing prompts (num_proc=32):  19%|â–ˆâ–‰        | 43/225 [00:00<00:01, 138.90 examples/s]Preparing prompts (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 64/225 [00:00<00:01, 146.31 examples/s]Preparing prompts (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 85/225 [00:00<00:00, 149.24 examples/s]Preparing prompts (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 113/225 [00:00<00:00, 184.00 examples/s]Preparing prompts (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 134/225 [00:00<00:00, 161.29 examples/s]Preparing prompts (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 155/225 [00:01<00:00, 143.39 examples/s]Preparing prompts (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 197/225 [00:01<00:00, 200.71 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:01<00:00, 166.16 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:13<01:35, 13.66s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:27<01:21, 13.53s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:39<01:04, 12.82s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:51<00:50, 12.63s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:07<00:41, 13.87s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:20<00:26, 13.43s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:32<00:13, 13.05s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:33<00:00,  9.13s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/225 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   4%|â–Ž         | 8/225 [00:00<00:05, 42.02 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 29/225 [00:00<00:01, 109.57 examples/s][A
Postprocessing dataset (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 57/225 [00:00<00:01, 161.53 examples/s][A
Postprocessing dataset (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–      | 78/225 [00:00<00:00, 160.98 examples/s][A
Postprocessing dataset (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 99/225 [00:00<00:00, 172.85 examples/s][A
Postprocessing dataset (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 127/225 [00:00<00:00, 177.33 examples/s][A
Postprocessing dataset (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 148/225 [00:00<00:00, 178.37 examples/s][A
Postprocessing dataset (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 169/225 [00:01<00:00, 178.88 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 197/225 [00:01<00:00, 197.72 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:01<00:00, 219.45 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:01<00:00, 169.59 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/225 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:00<00:00, 28389.34 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:00<00:00, 27961.20 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/225 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:00<00:00, 29086.71 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:00<00:00, 28603.59 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:35<00:00, 11.89s/it]
01/29/2025 04:04:49 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 153 examples [00:00, 21056.85 examples/s]
01/29/2025 04:04:49 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.44s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.41s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.42s/it]
Preparing prompts (num_proc=32):   0%|          | 0/153 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 5/153 [00:00<00:05, 27.97 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 20/153 [00:00<00:01, 71.84 examples/s]Preparing prompts (num_proc=32):  20%|â–ˆâ–‰        | 30/153 [00:00<00:01, 73.45 examples/s]Preparing prompts (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 60/153 [00:00<00:00, 131.33 examples/s]Preparing prompts (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 75/153 [00:00<00:00, 127.09 examples/s]Preparing prompts (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 90/153 [00:00<00:00, 115.51 examples/s]Preparing prompts (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 105/153 [00:00<00:00, 115.23 examples/s]Preparing prompts (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 125/153 [00:01<00:00, 129.45 examples/s]Preparing prompts (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/153 [00:01<00:00, 134.89 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:01<00:00, 112.40 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/5 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  20%|â–ˆâ–ˆ        | 1/5 [00:17<01:10, 17.54s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:29<00:42, 14.02s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:43<00:28, 14.16s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:55<00:13, 13.31s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:05<00:00, 12.20s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/153 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 5/153 [00:00<00:05, 26.33 examples/s][A
Postprocessing dataset (num_proc=32):  10%|â–‰         | 15/153 [00:00<00:02, 51.68 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 35/153 [00:00<00:01, 94.09 examples/s][A
Postprocessing dataset (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 50/153 [00:00<00:01, 100.32 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 70/153 [00:00<00:00, 127.29 examples/s][A
Postprocessing dataset (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 85/153 [00:00<00:00, 126.94 examples/s][A
Postprocessing dataset (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 105/153 [00:00<00:00, 134.43 examples/s][A
Postprocessing dataset (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 120/153 [00:01<00:00, 122.31 examples/s][A
Postprocessing dataset (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 145/153 [00:01<00:00, 146.32 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:01<00:00, 114.35 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/153 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:00<00:00, 19478.19 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:00<00:00, 19175.54 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/153 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:00<00:00, 20063.42 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:00<00:00, 19744.28 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:07<00:00, 13.56s/it]
01/29/2025 04:06:15 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 244 examples [00:00, 31352.56 examples/s]
01/29/2025 04:06:15 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.68s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.57s/it]
Preparing prompts (num_proc=32):   0%|          | 0/244 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/244 [00:00<00:06, 36.12 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 32/244 [00:00<00:01, 111.13 examples/s]Preparing prompts (num_proc=32):  20%|â–ˆâ–‰        | 48/244 [00:00<00:01, 116.08 examples/s]Preparing prompts (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 96/244 [00:00<00:00, 172.16 examples/s]Preparing prompts (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 128/244 [00:00<00:00, 207.35 examples/s]Preparing prompts (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 160/244 [00:00<00:00, 222.70 examples/s]Preparing prompts (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 188/244 [00:01<00:00, 209.27 examples/s]Preparing prompts (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 216/244 [00:01<00:00, 213.50 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 244/244 [00:01<00:00, 226.24 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 244/244 [00:01<00:00, 180.84 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:14<01:43, 14.72s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:28<01:24, 14.11s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:40<01:06, 13.38s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:57<00:58, 14.71s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:11<00:43, 14.38s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:26<00:29, 14.64s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:39<00:14, 14.09s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:51<00:00, 13.41s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/244 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/244 [00:00<00:05, 39.43 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–‹        | 40/244 [00:00<00:01, 127.91 examples/s][A
Postprocessing dataset (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 72/244 [00:00<00:01, 165.84 examples/s][A
Postprocessing dataset (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 96/244 [00:00<00:00, 183.37 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 120/244 [00:00<00:00, 198.07 examples/s][A
Postprocessing dataset (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 144/244 [00:00<00:00, 195.94 examples/s][A
Postprocessing dataset (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 174/244 [00:00<00:00, 197.91 examples/s][A
Postprocessing dataset (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 202/244 [00:01<00:00, 213.61 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 230/244 [00:01<00:00, 206.55 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 244/244 [00:01<00:00, 179.89 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/244 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 244/244 [00:00<00:00, 30164.18 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 244/244 [00:00<00:00, 29618.56 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/244 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 244/244 [00:00<00:00, 30645.61 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 244/244 [00:00<00:00, 30121.56 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:53<00:00, 14.21s/it]
01/29/2025 04:08:27 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 292 examples [00:00, 39799.07 examples/s]
01/29/2025 04:08:27 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.76s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.63s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.41s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.48s/it]
Preparing prompts (num_proc=32):   0%|          | 0/292 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/292 [00:00<00:06, 44.23 examples/s]Preparing prompts (num_proc=32):  10%|â–ˆ         | 30/292 [00:00<00:02, 91.50 examples/s]Preparing prompts (num_proc=32):  20%|â–ˆâ–‰        | 58/292 [00:00<00:01, 143.66 examples/s]Preparing prompts (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 85/292 [00:00<00:01, 177.90 examples/s]Preparing prompts (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 130/292 [00:00<00:00, 212.75 examples/s]Preparing prompts (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 166/292 [00:00<00:00, 228.72 examples/s]Preparing prompts (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 211/292 [00:01<00:00, 267.89 examples/s]Preparing prompts (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 247/292 [00:01<00:00, 235.73 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292/292 [00:01<00:00, 214.82 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:16<02:27, 16.38s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:07, 15.88s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:48<01:54, 16.33s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:05<01:39, 16.54s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:21<01:21, 16.34s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:05, 16.37s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:52<00:46, 15.62s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:05<00:29, 14.90s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:20<00:14, 14.92s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:27<00:00, 12.54s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/292 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/292 [00:00<00:05, 50.99 examples/s][A
Postprocessing dataset (num_proc=32):  14%|â–ˆâ–Ž        | 40/292 [00:00<00:01, 128.31 examples/s][A
Postprocessing dataset (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 85/292 [00:00<00:01, 194.71 examples/s][A
Postprocessing dataset (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 112/292 [00:00<00:00, 212.00 examples/s][A
Postprocessing dataset (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 139/292 [00:00<00:00, 217.56 examples/s][A
Postprocessing dataset (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 175/292 [00:00<00:00, 225.09 examples/s][A
Postprocessing dataset (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 202/292 [00:01<00:00, 210.28 examples/s][A
Postprocessing dataset (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 229/292 [00:01<00:00, 209.79 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 274/292 [00:01<00:00, 260.50 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292/292 [00:01<00:00, 208.86 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/292 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292/292 [00:00<00:00, 35587.30 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292/292 [00:00<00:00, 35046.55 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/292 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292/292 [00:00<00:00, 35791.13 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292/292 [00:00<00:00, 35224.96 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:29<00:00, 14.98s/it]
01/29/2025 04:11:15 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 42942.16 examples/s]
01/29/2025 04:11:15 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.59s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.45s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.48s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 54.61 examples/s]Preparing prompts (num_proc=32):  10%|â–ˆ         | 30/300 [00:00<00:02, 114.92 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 200.97 examples/s]Preparing prompts (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 220.60 examples/s]Preparing prompts (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 228.66 examples/s]Preparing prompts (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 165/300 [00:00<00:00, 226.61 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:01<00:00, 203.95 examples/s]Preparing prompts (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 231.75 examples/s]Preparing prompts (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 236.99 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 217.84 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:17<02:38, 17.65s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:32<02:06, 15.80s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:45, 15.08s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:24, 14.00s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:12<01:10, 14.08s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:26<00:55, 13.79s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:39<00:40, 13.65s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:53<00:27, 13.73s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:07<00:13, 13.75s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:13<00:00, 11.43s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 48.11 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 164.66 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 174.81 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 238.06 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 201.30 examples/s][A
Postprocessing dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 165/300 [00:00<00:00, 212.02 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 241.95 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 234.95 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 252.37 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 265.76 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 212.68 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 36929.28 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 36240.06 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 36699.85 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 36074.86 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:15<00:00, 13.57s/it]
01/29/2025 04:13:49 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 39077.37 examples/s]
01/29/2025 04:13:49 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.80s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.56s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:07, 39.59 examples/s]Preparing prompts (num_proc=32):  10%|â–ˆ         | 30/300 [00:00<00:02, 92.24 examples/s]Preparing prompts (num_proc=32):  20%|â–ˆâ–ˆ        | 60/300 [00:00<00:01, 147.26 examples/s]Preparing prompts (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 120/300 [00:00<00:00, 259.13 examples/s]Preparing prompts (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 156/300 [00:00<00:00, 244.96 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 262.33 examples/s]Preparing prompts (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 235.51 examples/s]Preparing prompts (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 273/300 [00:01<00:00, 263.03 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 219.42 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:16<02:30, 16.71s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:56, 14.62s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:44, 14.88s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:00<01:29, 14.95s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:12<01:10, 14.11s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:27<00:57, 14.28s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:42<00:44, 14.73s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:58<00:29, 14.83s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:12<00:14, 14.73s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:21<00:00, 12.85s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:07, 38.33 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 151.04 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 194.03 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 209.13 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 202.46 examples/s][A
Postprocessing dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 183/300 [00:00<00:00, 211.34 examples/s][A
Postprocessing dataset (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [00:01<00:00, 216.64 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 218.54 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [00:01<00:00, 269.28 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 206.53 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 38472.79 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37786.52 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37837.65 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37039.07 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:23<00:00, 14.35s/it]
01/29/2025 04:16:31 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 250 examples [00:00, 37228.43 examples/s]
01/29/2025 04:16:31 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.83s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.61s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.66s/it]
Preparing prompts (num_proc=32):   0%|          | 0/250 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/250 [00:00<00:06, 38.46 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 32/250 [00:00<00:01, 116.81 examples/s]Preparing prompts (num_proc=32):  22%|â–ˆâ–ˆâ–       | 56/250 [00:00<00:01, 159.25 examples/s]Preparing prompts (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 80/250 [00:00<00:00, 174.28 examples/s]Preparing prompts (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/250 [00:00<00:00, 197.69 examples/s]Preparing prompts (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 136/250 [00:00<00:00, 171.76 examples/s]Preparing prompts (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 160/250 [00:00<00:00, 187.31 examples/s]Preparing prompts (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 192/250 [00:01<00:00, 211.29 examples/s]Preparing prompts (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 222/250 [00:01<00:00, 206.93 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 182.31 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:13<01:36, 13.83s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:27<01:23, 13.90s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:40<01:06, 13.33s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:54<00:54, 13.69s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:06<00:38, 12.95s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:20<00:26, 13.34s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:31<00:12, 12.69s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:41<00:00, 11.69s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/250 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/250 [00:00<00:06, 39.17 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–Œ        | 40/250 [00:00<00:01, 152.64 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 64/250 [00:00<00:01, 169.20 examples/s][A
Postprocessing dataset (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/250 [00:00<00:00, 166.62 examples/s][A
Postprocessing dataset (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 120/250 [00:00<00:00, 190.48 examples/s][A
Postprocessing dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 152/250 [00:00<00:00, 191.04 examples/s][A
Postprocessing dataset (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 176/250 [00:00<00:00, 202.61 examples/s][A
Postprocessing dataset (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 200/250 [00:01<00:00, 198.90 examples/s][A
Postprocessing dataset (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 229/250 [00:01<00:00, 215.32 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 181.04 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/250 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 31582.66 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 31064.32 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/250 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 31438.73 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 30896.81 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:43<00:00, 12.93s/it]
01/29/2025 04:18:33 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 249 examples [00:00, 64017.51 examples/s]
01/29/2025 04:18:33 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:08,  4.31s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  4.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.63s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.59s/it]
Preparing prompts (num_proc=32):   0%|          | 0/249 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/249 [00:00<00:05, 46.59 examples/s]Preparing prompts (num_proc=32):   6%|â–‹         | 16/249 [00:00<00:04, 53.73 examples/s]Preparing prompts (num_proc=32):  22%|â–ˆâ–ˆâ–       | 56/249 [00:00<00:01, 161.90 examples/s]Preparing prompts (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 80/249 [00:00<00:01, 159.16 examples/s]Preparing prompts (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 104/249 [00:00<00:00, 171.65 examples/s]Preparing prompts (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 128/249 [00:00<00:00, 189.42 examples/s]Preparing prompts (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 160/249 [00:00<00:00, 205.47 examples/s]Preparing prompts (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 192/249 [00:01<00:00, 207.74 examples/s]Preparing prompts (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 214/249 [00:01<00:00, 195.06 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:01<00:00, 179.63 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:13<01:37, 13.99s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:27<01:21, 13.54s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:38<01:03, 12.73s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:51<00:50, 12.70s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:05<00:39, 13.20s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:18<00:26, 13.01s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:31<00:13, 13.18s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:41<00:00, 12.16s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/249 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/249 [00:00<00:05, 42.71 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 32/249 [00:00<00:01, 115.05 examples/s][A
Postprocessing dataset (num_proc=32):  22%|â–ˆâ–ˆâ–       | 56/249 [00:00<00:01, 157.88 examples/s][A
Postprocessing dataset (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 80/249 [00:00<00:00, 174.40 examples/s][A
Postprocessing dataset (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 104/249 [00:00<00:00, 192.86 examples/s][A
Postprocessing dataset (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 128/249 [00:00<00:00, 201.15 examples/s][A
Postprocessing dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 152/249 [00:00<00:00, 198.93 examples/s][A
Postprocessing dataset (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 176/249 [00:00<00:00, 210.55 examples/s][A
Postprocessing dataset (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 200/249 [00:01<00:00, 206.50 examples/s][A
Postprocessing dataset (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 228/249 [00:01<00:00, 213.81 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:01<00:00, 184.10 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/249 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:00<00:00, 29509.81 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:00<00:00, 29060.65 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/249 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:00<00:00, 30120.89 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:00<00:00, 29662.35 examples/s]
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:44<00:00, 13.01s/it]
01/29/2025 04:20:38 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 38184.42 examples/s]
01/29/2025 04:20:39 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.84s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.59s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.64s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:07, 36.35 examples/s]Preparing prompts (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 143.78 examples/s]Preparing prompts (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 175.41 examples/s]Preparing prompts (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 129/300 [00:00<00:00, 251.50 examples/s]Preparing prompts (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 165/300 [00:00<00:00, 232.87 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 235.88 examples/s]Preparing prompts (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 240.46 examples/s]Preparing prompts (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 269.64 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 282.02 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 218.40 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:15<02:23, 15.92s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:56, 14.50s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:43, 14.85s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.44s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:12<01:10, 14.16s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:25<00:55, 13.94s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:41<00:43, 14.42s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:54<00:28, 14.18s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:08<00:13, 13.93s/it]01/29/2025 04:23:02 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:23:02 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.97s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:16<00:00, 12.10s/it]Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 706, in <module>
    main()
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 655, in main
    save_checkpoint(split_output_dir, all_generated_ids, cur_step)
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 248, in save_checkpoint
    with open(output_path, "w") as file:
FileNotFoundError: [Errno 2] No such file or directory: 'processed3_parquet/45_Nhi/train/checkpoint-10.json'
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:16<00:00, 13.63s/it]
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:06<00:13,  6.58s/it]
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 706, in <module>
    main()
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 529, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4224, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4794, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 875, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 242, in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 331, in to
    return self._quantize(device)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 296, in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/bitsandbytes/functional.py", line 1211, in quantize_4bit
    out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 11.71 GiB of which 2.00 MiB is free. Process 111647 has 8.91 GiB memory in use. Including non-PyTorch memory, this process has 2.65 GiB memory in use. Of the allocated memory 2.46 GiB is allocated by PyTorch, and 19.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Running for speaker: Oanh (file: 46_Oanh.parquet)
Running for speaker: Huá»‡ (file: 25_Huá»‡.parquet)
Running for speaker: Thá»§y (file: 34_Thá»§y.parquet)
Running for speaker: HÃ¹ng (file: 32_HÃ¹ng.parquet)
Running for speaker: ThÃ¡i (file: 9_ThÃ¡i.parquet)
Running for speaker: Chiáº¿n (file: 29_Chiáº¿n.parquet)
Running for speaker: LÃ¢m (file: 39_LÃ¢m.parquet)
Running for speaker: Háº¡ (file: 40_Háº¡.parquet)
Running for speaker: My (file: 6_My.parquet)
Running for speaker: Quá»³nh (file: 13_Quá»³nh.parquet)
Running for speaker: KiÃªn (file: 8_KiÃªn.parquet)
Running for speaker: VÅ© (file: 20_VÅ©.parquet)
Running for speaker: Miu (file: 31_Miu.parquet)
Running for speaker: Hiáº¿c (file: 36_Hiáº¿c.parquet)
Running for speaker: HÆ°ng (file: 30_HÆ°ng.parquet)
Running for speaker: Nguyá»‡t (file: 27_Nguyá»‡t.parquet)
Running for speaker: PhÆ°Æ¡ng (file: 38_PhÆ°Æ¡ng.parquet)
Running for speaker: Ly (file: 16_Ly.parquet)
Running for speaker: NhÆ° (file: 14_NhÆ°.parquet)
Running for speaker: Huy (file: 4_Huy.parquet)
Running for speaker: Vy (file: 2_Vy.parquet)
Running for speaker: Nhi (file: 45_Nhi.parquet)
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/scripts/execute_prompt_creation.py", line 45, in <module>
    # Cháº¡y lá»‡nh
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python', './scripts/run_prompt_creation.py', '--speaker_name', 'Nhi', '--is_single_speaker', '--is_new_speaker_prompt', '--dataset_name', 'processed2_parquet/45_Nhi.parquet', '--model_name_or_path', 'mistralai/Mistral-7B-Instruct-v0.2', '--per_device_eval_batch_size', '32', '--attn_implementation', 'sdpa', '--output_dir', 'processed3_parquet/45_Nhi', '--load_in_4bit', '--preprocessing_num_workers', '32', '--dataloader_num_workers', '32']' returned non-zero exit status 1.
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/scripts/execute_prompt_creation.py", line 46, in <module>
    subprocess.run(command, check=True)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python', './scripts/run_prompt_creation.py', '--speaker_name', 'Diá»‡p', '--is_single_speaker', '--is_new_speaker_prompt', '--dataset_name', 'processed2_parquet/41_Diá»‡p.parquet', '--model_name_or_path', 'mistralai/Mistral-7B-Instruct-v0.2', '--per_device_eval_batch_size', '32', '--attn_implementation', 'sdpa', '--output_dir', 'processed3_parquet/41_Diá»‡p', '--load_in_4bit', '--preprocessing_num_workers', '32', '--dataloader_num_workers', '32', '--save_only_parquet']' returned non-zero exit status 1.
01/29/2025 04:23:26 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:23:26 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.07s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.81s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.66s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:15<02:22, 15.88s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:02, 15.30s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:46<01:49, 15.71s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:01<01:31, 15.22s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:16<01:14, 15.00s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:31<01:00, 15.16s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:44<00:43, 14.51s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:58<00:28, 14.13s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:13<00:14, 14.65s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:20<00:00, 12.24s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 49.82 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 138.08 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 190.70 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 201.29 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 220.23 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 203.28 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:01<00:00, 212.22 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 220.10 examples/s][A
Postprocessing dataset (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [00:01<00:00, 222.99 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 291/300 [00:01<00:00, 252.55 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 204.08 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 32146.01 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 31595.51 examples/s]
01/29/2025 04:25:59 - INFO - __main__ - Saving only to Parquet format...
01/29/2025 04:25:59 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/41_Diá»‡p/parquet_files/train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:22<00:00, 14.30s/it]
01/29/2025 04:26:03 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:26:03 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.69s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:11,  5.72s/it]
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 706, in <module>
    main()
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 529, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4224, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4794, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 817, in _load_state_dict_into_meta_model
    param = param.to(dtype)
KeyboardInterrupt
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Running for speaker: Oanh (file: 46_Oanh.parquet)
Traceback (most recent call last):
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/scripts/execute_prompt_creation.py", line 46, in <module>
    subprocess.run(command, check=True)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1222, in wait
    self._wait(timeout=sigint_timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
KeyboardInterrupt
01/29/2025 04:27:40 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:27:40 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.03s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.84s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.67s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:14<02:13, 14.81s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:51, 13.90s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:41<01:36, 13.81s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:54<01:20, 13.48s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:10<01:10, 14.15s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:25<00:57, 14.45s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:38<00:42, 14.07s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:52<00:28, 14.20s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:10<00:15, 15.12s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:16<00:00, 12.56s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:07, 41.41 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 163.25 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 175.17 examples/s][A
Postprocessing dataset (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 120/300 [00:00<00:00, 211.93 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 215.03 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 203.04 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:01<00:00, 206.94 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 229.06 examples/s][A
Postprocessing dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 273/300 [00:01<00:00, 250.83 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 205.98 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 32040.42 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 31488.77 examples/s]
01/29/2025 04:30:09 - INFO - __main__ - Saving only to Parquet format with original file name...
01/29/2025 04:30:09 - INFO - __main__ - Saved dataset to Parquet: processed3_parquet/41_Diá»‡p/41_Diá»‡p.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:19<00:00, 13.91s/it]
01/29/2025 04:30:13 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:30:13 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.53s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.46s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 711, in <module>
    main()
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 645, in main
    generated_ids = generate_step(batch)
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 602, in generate_step
    output_ids = accelerator.unwrap_model(model).generate(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/generation/utils.py", line 2255, in generate
    result = self._sample(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/generation/utils.py", line 3243, in _sample
    while self._has_unfinished_sequences(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/generation/utils.py", line 2453, in _has_unfinished_sequences
    elif this_peer_finished:
KeyboardInterrupt
 ... :   0%|          | 0/10 [00:08<?, ?it/s]
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Running for speaker: Oanh (file: 46_Oanh.parquet)
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/scripts/execute_prompt_creation.py", line 46, in <module>
    subprocess.run(command, check=True)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
01/29/2025 04:33:08 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:33:08 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.69s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.45s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.48s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]01/29/2025 04:33:20 - INFO - __main__ - Json file processed3_parquet/41_Diá»‡p/train/checkpoint-10.json loaded.
01/29/2025 04:33:20 - INFO - __main__ - Resuming train from step 10

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 32128.77 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 31632.84 examples/s]
01/29/2025 04:33:20 - INFO - __main__ - Saving only to Parquet format with original file name...
01/29/2025 04:33:20 - INFO - __main__ - Saved dataset to Parquet: processed3_parquet/41_Diá»‡p/41_Diá»‡p.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 60.17it/s]
01/29/2025 04:33:24 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:33:24 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.80s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.65s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.51s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.56s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:13<02:02, 13.65s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:26<01:45, 13.25s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:37<01:26, 12.33s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:51<01:17, 12.87s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:04<01:04, 12.81s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:15<00:49, 12.43s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:27<00:36, 12.28s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:40<00:24, 12.23s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:52<00:12, 12.29s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:00<00:00, 11.10s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/299 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/299 [00:00<00:06, 43.11 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/299 [00:00<00:01, 130.93 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/299 [00:00<00:01, 174.00 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/299 [00:00<00:00, 232.73 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 137/299 [00:00<00:00, 211.53 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 173/299 [00:00<00:00, 202.22 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 227/299 [00:01<00:00, 259.25 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 263/299 [00:01<00:00, 245.09 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 257.91 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 209.04 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/299 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:00<00:00, 33024.28 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:00<00:00, 32536.76 examples/s]
01/29/2025 04:35:40 - INFO - __main__ - Saving only to Parquet format with original file name...
01/29/2025 04:35:40 - INFO - __main__ - Saved dataset to Parquet: processed3_parquet/46_Oanh/46_Oanh.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:03<00:00, 12.32s/it]
01/29/2025 04:35:44 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:35:44 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.66s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.47s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/6 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  17%|â–ˆâ–‹        | 1/6 [00:13<01:05, 13.17s/it] ... :  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:27<00:55, 13.91s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:40<00:40, 13.66s/it]Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 715, in <module>
    main()
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 645, in main
    generated_ids = generate_step(batch)
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 602, in generate_step
    output_ids = accelerator.unwrap_model(model).generate(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/generation/utils.py", line 2255, in generate
    result = self._sample(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/generation/utils.py", line 3313, in _sample
    unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/generation/stopping_criteria.py", line 496, in __call__
    is_done = is_done | criteria(input_ids, scores, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/generation/stopping_criteria.py", line 466, in __call__
    is_done = isin_mps_friendly(input_ids[:, -1], self.eos_token_id)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/pytorch_utils.py", line 338, in isin_mps_friendly
    return torch.isin(elements, test_elements)
KeyboardInterrupt
 ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:50<00:50, 16.90s/it]
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Running for speaker: Oanh (file: 46_Oanh.parquet)
Running for speaker: Huá»‡ (file: 25_Huá»‡.parquet)
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/scripts/execute_prompt_creation.py", line 46, in <module>
    subprocess.run(command, check=True)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
01/29/2025 04:37:58 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:37:58 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.76s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:04,  4.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.12s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:14<02:08, 14.29s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:51, 13.94s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:40, 14.36s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:57<01:26, 14.45s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:16, 15.29s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:01, 15.25s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:43<00:44, 14.80s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:59<00:30, 15.12s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:13<00:14, 14.92s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:23<00:00, 13.38s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 47.70 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 155.58 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 197.42 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 208.91 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 225.02 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 242.75 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:01<00:00, 215.04 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 212.50 examples/s][A
Postprocessing dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 273/300 [00:01<00:00, 245.93 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 207.61 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 36731.99 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 36157.79 examples/s]
01/29/2025 04:40:38 - INFO - __main__ - Saving only to Parquet format with original file name...
01/29/2025 04:40:38 - INFO - __main__ - Saved dataset to Parquet: processed3_parquet/41_Diá»‡p/41_Diá»‡p.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:25<00:00, 14.57s/it]
01/29/2025 04:40:42 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:40:42 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:08,  4.23s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:06<00:12,  6.48s/it]
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 715, in <module>
    main()
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 529, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4224, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4794, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 875, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 242, in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 331, in to
    return self._quantize(device)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 295, in _quantize
    w = self.data.contiguous().to(device)
KeyboardInterrupt
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Running for speaker: Oanh (file: 46_Oanh.parquet)
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/scripts/execute_prompt_creation.py", line 46, in <module>
    subprocess.run(command, check=True)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
01/29/2025 04:41:36 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:41:36 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.01s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.75s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:14<02:13, 14.79s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:30<02:04, 15.52s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:42, 14.70s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:00<01:31, 15.18s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:13, 14.61s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:30<01:01, 15.35s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:44<00:44, 14.78s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:57<00:28, 14.36s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:11<00:14, 14.22s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:19<00:00, 12.19s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 48.14 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 154.12 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 194.02 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 208.74 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 237.91 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 236.08 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 217.80 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 196.78 examples/s][A
Postprocessing dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 273/300 [00:01<00:00, 248.80 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 208.72 examples/s]
01/29/2025 04:44:08 - INFO - __main__ - Saving only to Parquet format with original file name...
01/29/2025 04:44:08 - INFO - __main__ - Saved dataset to Parquet: processed3_parquet/41_Diá»‡p/41_Diá»‡p.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:21<00:00, 14.18s/it]
01/29/2025 04:44:12 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 04:44:12 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.73s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:10,  5.34s/it]
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 715, in <module>
    main()
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 529, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4224, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4794, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 817, in _load_state_dict_into_meta_model
    param = param.to(dtype)
KeyboardInterrupt
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Running for speaker: Oanh (file: 46_Oanh.parquet)
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/scripts/execute_prompt_creation.py", line 46, in <module>
    subprocess.run(command, check=True)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
01/29/2025 05:00:12 - INFO - __main__ - *** Load annotated dataset ***
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 691, in <module>
    main()
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 488, in main
    raw_datasets_features = set(raw_datasets[next(iter(raw_datasets))].features.keys())
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2780, in __getitem__
    return self._getitem(key)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2764, in _getitem
    pa_subtable = query_table(self._data, key, indices=self._indices)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 593, in query_table
    _check_valid_index_key(key, size)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 543, in _check_valid_index_key
    _check_valid_index_key(int(max(key)), size=size)
ValueError: invalid literal for int() with base 10: 'utterance_pitch_std'
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/scripts/execute_prompt_creation.py", line 46, in <module>
    subprocess.run(command, check=True)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python', './scripts/run_prompt_creation.py', '--speaker_name', 'Diá»‡p', '--is_single_speaker', '--is_new_speaker_prompt', '--dataset_name', 'processed2_parquet/41_Diá»‡p.parquet', '--model_name_or_path', 'mistralai/Mistral-7B-Instruct-v0.2', '--per_device_eval_batch_size', '32', '--attn_implementation', 'sdpa', '--output_dir', 'processed3_parquet/41_Diá»‡p', '--load_in_4bit', '--preprocessing_num_workers', '32', '--dataloader_num_workers', '32', '--save_only_parquet']' returned non-zero exit status 1.
01/29/2025 05:07:13 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:07:13 - INFO - __main__ - *** Load pretrained model ***
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 703, in <module>
    main()
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 528, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 487, in from_pretrained
    resolved_config_file = cached_file(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 860, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 923, in _hf_hub_download_to_cache_dir
    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1374, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1294, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 278, in _request_wrapper
    response = _request_wrapper(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 301, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 93, in send
    return super().send(request, *args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/http/client.py", line 1375, in getresponse
    response.begin()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/socket.py", line 717, in readinto
    return self._sock.recv_into(b)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/ssl.py", line 1307, in recv_into
    return self.read(nbytes, buffer)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/ssl.py", line 1163, in read
    return self._sslobj.read(len, buffer)
KeyboardInterrupt
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/scripts/execute_prompt_creation.py", line 46, in <module>
    subprocess.run(command, check=True)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
01/29/2025 05:07:23 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:07:23 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.96s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.55s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.76s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:16<02:29, 16.65s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:55, 14.49s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:40, 14.32s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:00<01:32, 15.35s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:13, 14.71s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:31<01:01, 15.46s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:46<00:46, 15.41s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:00<00:30, 15.12s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:15<00:15, 15.06s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:22<00:00, 12.43s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 48.00 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 178.47 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 202.99 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 199.54 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 216.63 examples/s][A
Postprocessing dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 165/300 [00:00<00:00, 209.14 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 239.15 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 216.67 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 236.52 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 254.52 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 208.10 examples/s]

Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 38104.63 examples/s][ASaving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 37517.25 examples/s]
01/29/2025 05:09:57 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/41_Diá»‡p/parquet_files/train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:24<00:00, 14.47s/it]
01/29/2025 05:10:01 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:10:01 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.58s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.66s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:08<00:04,  4.22s/it]
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 703, in <module>
    main()
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 528, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4224, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4794, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/modeling_utils.py", line 817, in _load_state_dict_into_meta_model
    param = param.to(dtype)
KeyboardInterrupt
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Running for speaker: Oanh (file: 46_Oanh.parquet)
Traceback (most recent call last):
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/scripts/execute_prompt_creation.py", line 46, in <module>
    subprocess.run(command, check=True)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1222, in wait
    self._wait(timeout=sigint_timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
KeyboardInterrupt
01/29/2025 05:20:31 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:20:31 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.01s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.25s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]01/29/2025 05:20:43 - INFO - __main__ - Json file processed3_parquet/41_Diá»‡p/train/checkpoint-10.json loaded.
01/29/2025 05:20:43 - INFO - __main__ - Resuming train from step 10
01/29/2025 05:20:43 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/41_Diá»‡p/parquet_files/41_Diá»‡p_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 63.94it/s]
01/29/2025 05:20:47 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:20:47 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.30s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.61s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.55s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 706, in <module>
    main()
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 644, in main
    generated_ids = generate_step(batch)
  File "/home/pc/Desktop/dataspeech/dataspeech/./scripts/run_prompt_creation.py", line 601, in generate_step
    output_ids = accelerator.unwrap_model(model).generate(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/generation/utils.py", line 2255, in generate
    result = self._sample(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/generation/utils.py", line 3257, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 832, in forward
    outputs = self.model(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 561, in forward
    layer_outputs = decoder_layer(
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 261, in forward
    hidden_states = self.post_attention_layernorm(hidden_states)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
KeyboardInterrupt
 ... :   0%|          | 0/10 [00:07<?, ?it/s]
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Running for speaker: Oanh (file: 46_Oanh.parquet)
Traceback (most recent call last):
  File "/home/pc/Desktop/dataspeech/dataspeech/scripts/execute_prompt_creation.py", line 46, in <module>
    subprocess.run(command, check=True)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/pc/anaconda3/envs/dataspeech/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
01/29/2025 05:25:49 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:25:49 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.81s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.55s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:15<02:22, 15.86s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:53, 14.16s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:45, 15.01s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:59<01:28, 14.73s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:13, 14.78s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:28<00:58, 14.71s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:40<00:41, 13.87s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:54<00:27, 13.87s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:09<00:14, 14.20s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:18<00:00, 12.60s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 50.69 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:02, 120.40 examples/s][A
Postprocessing dataset (num_proc=32):  30%|â–ˆâ–ˆâ–ˆ       | 90/300 [00:00<00:01, 198.39 examples/s][A
Postprocessing dataset (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 129/300 [00:00<00:00, 243.78 examples/s][A
Postprocessing dataset (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 156/300 [00:00<00:00, 218.73 examples/s][A
Postprocessing dataset (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 192/300 [00:00<00:00, 238.59 examples/s][A
Postprocessing dataset (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 219/300 [00:01<00:00, 219.63 examples/s][A
Postprocessing dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 246/300 [00:01<00:00, 221.28 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [00:01<00:00, 242.66 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 207.63 examples/s]
01/29/2025 05:28:22 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/41_Diá»‡p/parquet_files/41_Diá»‡p_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:20<00:00, 14.08s/it]
01/29/2025 05:28:26 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:28:26 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.44s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.37s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.14s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:12<01:56, 12.95s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:52, 14.00s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:39<01:30, 12.95s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:54<01:23, 13.84s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:08<01:10, 14.01s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:20<00:53, 13.32s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:32<00:37, 12.61s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:42<00:24, 12.04s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:55<00:12, 12.21s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:02<00:00, 10.53s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/299 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/299 [00:00<00:05, 49.94 examples/s][A
Postprocessing dataset (num_proc=32):  10%|â–ˆ         | 30/299 [00:00<00:02, 110.01 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/299 [00:00<00:01, 190.90 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/299 [00:00<00:00, 228.50 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 137/299 [00:00<00:00, 213.22 examples/s][A
Postprocessing dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 164/299 [00:00<00:00, 219.01 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 200/299 [00:00<00:00, 232.20 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 227/299 [00:01<00:00, 237.17 examples/s][A
Postprocessing dataset (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 254/299 [00:01<00:00, 228.50 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 271.52 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 211.79 examples/s]
01/29/2025 05:30:41 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/46_Oanh/parquet_files/46_Oanh_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:04<00:00, 12.44s/it]
01/29/2025 05:30:45 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:30:45 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.58s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.51s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.53s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/6 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  17%|â–ˆâ–‹        | 1/6 [00:13<01:05, 13.06s/it] ... :  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:27<00:55, 13.95s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:42<00:42, 14.28s/it] ... :  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:58<00:29, 14.89s/it] ... :  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [01:14<00:15, 15.34s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:25<00:00, 13.82s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/180 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 6/180 [00:00<00:05, 32.17 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 24/180 [00:00<00:01, 83.74 examples/s][A
Postprocessing dataset (num_proc=32):  30%|â–ˆâ–ˆâ–ˆ       | 54/180 [00:00<00:00, 143.93 examples/s][A
Postprocessing dataset (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 72/180 [00:00<00:00, 150.52 examples/s][A
Postprocessing dataset (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/180 [00:00<00:00, 147.08 examples/s][A
Postprocessing dataset (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/180 [00:00<00:00, 136.63 examples/s][A
Postprocessing dataset (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 135/180 [00:00<00:00, 151.75 examples/s][A
Postprocessing dataset (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 155/180 [00:01<00:00, 155.28 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 175/180 [00:01<00:00, 157.17 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:01<00:00, 135.67 examples/s]
01/29/2025 05:32:21 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/25_Huá»‡/parquet_files/25_Huá»‡_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:27<00:00, 14.54s/it]
01/29/2025 05:32:25 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:32:25 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.73s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.60s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:16<02:25, 16.21s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:55, 14.49s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:45<01:45, 15.01s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:25, 14.22s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:12<01:11, 14.31s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:27<00:58, 14.56s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:40<00:42, 14.01s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:54<00:27, 13.88s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:07<00:13, 13.74s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:17<00:00, 12.42s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 51.52 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 138.55 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 181.08 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 209.19 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 212.59 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 244.31 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 228.57 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 226.98 examples/s][A
Postprocessing dataset (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [00:01<00:00, 232.08 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [00:01<00:00, 227.10 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 206.41 examples/s]
01/29/2025 05:34:53 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/34_Thá»§y/parquet_files/34_Thá»§y_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:19<00:00, 13.94s/it]
01/29/2025 05:34:57 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:34:57 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.85s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.81s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.79s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/7 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  14%|â–ˆâ–        | 1/7 [00:12<01:13, 12.25s/it] ... :  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:23<00:57, 11.42s/it] ... :  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:34<00:45, 11.42s/it] ... :  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:45<00:34, 11.38s/it] ... :  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:57<00:22, 11.44s/it] ... :  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [01:09<00:11, 11.58s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:18<00:00, 10.73s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/214 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 7/214 [00:00<00:05, 37.46 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 28/214 [00:00<00:02, 90.17 examples/s][A
Postprocessing dataset (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 63/214 [00:00<00:00, 154.55 examples/s][A
Postprocessing dataset (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 91/214 [00:00<00:00, 159.88 examples/s][A
Postprocessing dataset (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 119/214 [00:00<00:00, 172.91 examples/s][A
Postprocessing dataset (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 140/214 [00:00<00:00, 179.66 examples/s][A
Postprocessing dataset (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 160/214 [00:01<00:00, 162.70 examples/s][A
Postprocessing dataset (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 178/214 [00:01<00:00, 165.84 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 208/214 [00:01<00:00, 196.12 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214/214 [00:01<00:00, 158.98 examples/s]
01/29/2025 05:36:27 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/32_HÃ¹ng/parquet_files/32_HÃ¹ng_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:20<00:00, 11.48s/it]
01/29/2025 05:36:31 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:36:31 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.86s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.60s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:14<01:43, 14.84s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:30<01:31, 15.24s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:44<01:13, 14.62s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [01:01<01:03, 15.78s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:16<00:45, 15.31s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:30<00:29, 14.78s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:46<00:15, 15.20s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:57<00:00, 13.88s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/243 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/243 [00:00<00:06, 38.73 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–‹        | 40/243 [00:00<00:01, 132.07 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–‹       | 64/243 [00:00<00:01, 158.89 examples/s][A
Postprocessing dataset (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/243 [00:00<00:00, 177.98 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 112/243 [00:00<00:00, 179.43 examples/s][A
Postprocessing dataset (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 144/243 [00:00<00:00, 205.15 examples/s][A
Postprocessing dataset (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 166/243 [00:00<00:00, 192.19 examples/s][A
Postprocessing dataset (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 187/243 [00:01<00:00, 167.68 examples/s][A
Postprocessing dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 222/243 [00:01<00:00, 206.95 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [00:01<00:00, 174.70 examples/s]
01/29/2025 05:38:40 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/9_ThÃ¡i/parquet_files/9_ThÃ¡i_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:59<00:00, 14.92s/it]
01/29/2025 05:38:44 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:38:44 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.71s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.55s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.65s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.64s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/7 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  14%|â–ˆâ–        | 1/7 [00:11<01:11, 11.85s/it] ... :  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:23<00:57, 11.51s/it] ... :  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:34<00:46, 11.66s/it] ... :  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:48<00:37, 12.50s/it] ... :  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [01:01<00:25, 12.50s/it] ... :  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [01:12<00:12, 12.23s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:18<00:00, 10.06s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/200 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   4%|â–Ž         | 7/200 [00:00<00:05, 34.30 examples/s][A
Postprocessing dataset (num_proc=32):  18%|â–ˆâ–Š        | 35/200 [00:00<00:01, 118.33 examples/s][A
Postprocessing dataset (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 56/200 [00:00<00:01, 127.84 examples/s][A
Postprocessing dataset (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [00:00<00:00, 158.02 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:00<00:00, 144.15 examples/s][A
Postprocessing dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [00:00<00:00, 146.90 examples/s][A
Postprocessing dataset (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [00:01<00:00, 168.38 examples/s][A
Postprocessing dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [00:01<00:00, 162.96 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [00:01<00:00, 179.43 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:01<00:00, 148.73 examples/s]
01/29/2025 05:40:14 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/29_Chiáº¿n/parquet_files/29_Chiáº¿n_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:20<00:00, 11.52s/it]
01/29/2025 05:40:18 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:40:18 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.04s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.83s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/6 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  17%|â–ˆâ–‹        | 1/6 [00:12<01:04, 13.00s/it] ... :  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:24<00:48, 12.19s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:39<00:40, 13.49s/it] ... :  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:50<00:25, 12.61s/it] ... :  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [01:02<00:12, 12.36s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:11<00:00, 11.11s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/184 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 6/184 [00:00<00:05, 32.79 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 24/184 [00:00<00:01, 97.25 examples/s][A
Postprocessing dataset (num_proc=32):  20%|â–ˆâ–‰        | 36/184 [00:00<00:01, 102.39 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 48/184 [00:00<00:01, 104.31 examples/s][A
Postprocessing dataset (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 72/184 [00:00<00:00, 141.12 examples/s][A
Postprocessing dataset (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 96/184 [00:00<00:00, 153.38 examples/s][A
Postprocessing dataset (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 114/184 [00:00<00:00, 150.75 examples/s][A
Postprocessing dataset (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 138/184 [00:00<00:00, 166.88 examples/s][A
Postprocessing dataset (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 159/184 [00:01<00:00, 157.35 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:01<00:00, 177.56 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:01<00:00, 138.40 examples/s]
01/29/2025 05:41:41 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/39_LÃ¢m/parquet_files/39_LÃ¢m_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:13<00:00, 12.28s/it]
01/29/2025 05:41:45 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:41:45 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.67s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.55s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:15<02:20, 15.64s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:50, 13.82s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:41<01:33, 13.42s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:56<01:25, 14.32s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:12, 14.55s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:26<00:57, 14.45s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:40<00:43, 14.52s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:54<00:28, 14.43s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:07<00:13, 13.92s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:14<00:00, 11.70s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 47.80 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 159.56 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 193.96 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 210.92 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 227.25 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 199.63 examples/s][A
Postprocessing dataset (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 219/300 [00:01<00:00, 228.14 examples/s][A
Postprocessing dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 246/300 [00:01<00:00, 231.05 examples/s][A
Postprocessing dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 273/300 [00:01<00:00, 229.63 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 210.71 examples/s]
01/29/2025 05:44:11 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/40_Háº¡/parquet_files/40_Háº¡_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:16<00:00, 13.67s/it]
01/29/2025 05:44:15 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:44:15 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.02s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.93s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.89s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:14<01:42, 14.69s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:28<01:26, 14.39s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:44<01:15, 15.16s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:57<00:57, 14.31s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:11<00:42, 14.01s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:24<00:27, 13.82s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:37<00:13, 13.56s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:49<00:00, 13.07s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/250 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/250 [00:00<00:05, 41.70 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–Œ        | 40/250 [00:00<00:01, 142.60 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 64/250 [00:00<00:01, 158.50 examples/s][A
Postprocessing dataset (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/250 [00:00<00:00, 175.98 examples/s][A
Postprocessing dataset (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 120/250 [00:00<00:00, 202.68 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 144/250 [00:00<00:00, 200.61 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 168/250 [00:00<00:00, 205.80 examples/s][A
Postprocessing dataset (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 192/250 [00:01<00:00, 210.38 examples/s][A
Postprocessing dataset (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 215/250 [00:01<00:00, 186.18 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 226.81 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 182.88 examples/s]
01/29/2025 05:46:17 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/6_My/parquet_files/6_My_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:52<00:00, 14.01s/it]
01/29/2025 05:46:21 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:46:21 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.69s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.63s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.61s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:13<02:02, 13.57s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:49, 13.66s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:39, 14.23s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:55<01:23, 13.98s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:10<01:11, 14.26s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:23<00:55, 13.78s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:37<00:41, 13.72s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:51<00:28, 14.05s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:06<00:14, 14.41s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:13<00:00, 11.96s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 49.54 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 151.78 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 185.17 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 218.02 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 198.19 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 235.63 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 229.37 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 234.64 examples/s][A
Postprocessing dataset (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [00:01<00:00, 225.26 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 291/300 [00:01<00:00, 259.84 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 212.04 examples/s]
01/29/2025 05:48:46 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/13_Quá»³nh/parquet_files/13_Quá»³nh_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:15<00:00, 13.56s/it]
01/29/2025 05:48:49 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:48:49 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.09s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:02,  2.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.93s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:10<01:16, 10.93s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:21<01:03, 10.58s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:35<01:00, 12.12s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:45<00:45, 11.31s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:58<00:35, 11.89s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:11<00:24, 12.30s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:24<00:12, 12.46s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:34<00:00, 11.81s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/249 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/249 [00:00<00:05, 40.78 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 32/249 [00:00<00:01, 120.33 examples/s][A
Postprocessing dataset (num_proc=32):  22%|â–ˆâ–ˆâ–       | 56/249 [00:00<00:01, 154.01 examples/s][A
Postprocessing dataset (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/249 [00:00<00:00, 172.49 examples/s][A
Postprocessing dataset (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 120/249 [00:00<00:00, 205.15 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 144/249 [00:00<00:00, 192.44 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 168/249 [00:00<00:00, 200.23 examples/s][A
Postprocessing dataset (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 192/249 [00:01<00:00, 204.09 examples/s][A
Postprocessing dataset (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 214/249 [00:01<00:00, 200.35 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:01<00:00, 226.89 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:01<00:00, 181.22 examples/s]
01/29/2025 05:50:38 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/8_KiÃªn/parquet_files/8_KiÃªn_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:36<00:00, 12.09s/it]
01/29/2025 05:50:42 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:50:42 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.70s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.61s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.53s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:12<01:51, 12.39s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:24<01:36, 12.11s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:35<01:22, 11.76s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:49<01:15, 12.56s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:01<01:01, 12.23s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:12<00:48, 12.03s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:24<00:35, 11.85s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:34<00:22, 11.37s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:46<00:11, 11.41s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:51<00:00,  9.58s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 52.09 examples/s][A
Postprocessing dataset (num_proc=32):  10%|â–ˆ         | 30/300 [00:00<00:02, 104.95 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 187.55 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 244.09 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 206.70 examples/s][A
Postprocessing dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 165/300 [00:00<00:00, 193.67 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:01<00:00, 223.28 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 232.79 examples/s][A
Postprocessing dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 273/300 [00:01<00:00, 249.48 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 214.90 examples/s]
01/29/2025 05:52:45 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/20_VÅ©/parquet_files/20_VÅ©_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:53<00:00, 11.37s/it]
01/29/2025 05:52:49 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:52:49 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:08,  4.01s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.65s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/9 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  11%|â–ˆ         | 1/9 [00:13<01:50, 13.83s/it] ... :  22%|â–ˆâ–ˆâ–       | 2/9 [00:25<01:29, 12.79s/it] ... :  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:37<01:14, 12.39s/it] ... :  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:52<01:07, 13.41s/it] ... :  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [01:04<00:50, 12.72s/it] ... :  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [01:15<00:36, 12.25s/it] ... :  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [01:29<00:25, 12.81s/it] ... :  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:40<00:12, 12.25s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:51<00:00, 11.79s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/275 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 9/275 [00:00<00:05, 46.79 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–‹        | 45/275 [00:00<00:01, 145.47 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 72/275 [00:00<00:01, 178.70 examples/s][A
Postprocessing dataset (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 99/275 [00:00<00:00, 200.10 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 135/275 [00:00<00:00, 228.06 examples/s][A
Postprocessing dataset (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 162/275 [00:00<00:00, 213.78 examples/s][A
Postprocessing dataset (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 187/275 [00:00<00:00, 212.33 examples/s][A
Postprocessing dataset (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 211/275 [00:01<00:00, 217.68 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 243/275 [00:01<00:00, 227.73 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275/275 [00:01<00:00, 240.16 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275/275 [00:01<00:00, 198.02 examples/s]
01/29/2025 05:54:55 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/31_Miu/parquet_files/31_Miu_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:53<00:00, 12.62s/it]
01/29/2025 05:54:59 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:54:59 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.65s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.52s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:13<01:37, 13.97s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:24<01:11, 11.96s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:36<00:58, 11.78s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:50<00:50, 12.73s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:04<00:40, 13.36s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:16<00:25, 12.75s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:28<00:12, 12.69s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:29<00:00,  8.98s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/225 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   4%|â–Ž         | 8/225 [00:00<00:05, 38.55 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–Œ        | 36/225 [00:00<00:01, 116.49 examples/s][A
Postprocessing dataset (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 57/225 [00:00<00:01, 139.88 examples/s][A
Postprocessing dataset (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 85/225 [00:00<00:00, 164.61 examples/s][A
Postprocessing dataset (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 113/225 [00:00<00:00, 182.92 examples/s][A
Postprocessing dataset (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 134/225 [00:00<00:00, 182.58 examples/s][A
Postprocessing dataset (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 155/225 [00:00<00:00, 167.18 examples/s][A
Postprocessing dataset (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 183/225 [00:01<00:00, 191.94 examples/s][A
Postprocessing dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 204/225 [00:01<00:00, 184.44 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:01<00:00, 163.42 examples/s]
01/29/2025 05:56:40 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/36_Hiáº¿c/parquet_files/36_Hiáº¿c_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:32<00:00, 11.50s/it]
01/29/2025 05:56:43 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:56:43 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.23s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.31s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/5 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  20%|â–ˆâ–ˆ        | 1/5 [00:16<01:06, 16.67s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:28<00:41, 13.91s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:39<00:25, 12.51s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:52<00:12, 12.64s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:03<00:00, 12.10s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/153 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 5/153 [00:00<00:05, 27.12 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 20/153 [00:00<00:01, 73.92 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 35/153 [00:00<00:01, 101.24 examples/s][A
Postprocessing dataset (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 50/153 [00:00<00:00, 104.91 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 70/153 [00:00<00:00, 127.41 examples/s][A
Postprocessing dataset (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 85/153 [00:00<00:00, 126.77 examples/s][A
Postprocessing dataset (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 105/153 [00:00<00:00, 130.35 examples/s][A
Postprocessing dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 125/153 [00:01<00:00, 133.98 examples/s][A
Postprocessing dataset (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/153 [00:01<00:00, 136.77 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:01<00:00, 115.96 examples/s]
01/29/2025 05:58:00 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/30_HÆ°ng/parquet_files/30_HÆ°ng_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:05<00:00, 13.12s/it]
01/29/2025 05:58:04 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 05:58:04 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.49s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.66s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:15<01:51, 15.91s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:27<01:20, 13.40s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:40<01:05, 13.19s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:54<00:54, 13.56s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:07<00:40, 13.43s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:20<00:26, 13.07s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:31<00:12, 12.65s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:41<00:00, 11.60s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/244 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/244 [00:00<00:06, 35.55 examples/s][A
Postprocessing dataset (num_proc=32):  20%|â–ˆâ–‰        | 48/244 [00:00<00:01, 160.82 examples/s][A
Postprocessing dataset (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 72/244 [00:00<00:01, 148.71 examples/s][A
Postprocessing dataset (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 104/244 [00:00<00:00, 189.53 examples/s][A
Postprocessing dataset (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 128/244 [00:00<00:00, 162.87 examples/s][A
Postprocessing dataset (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 160/244 [00:00<00:00, 197.69 examples/s][A
Postprocessing dataset (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 188/244 [00:01<00:00, 204.47 examples/s][A
Postprocessing dataset (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 216/244 [00:01<00:00, 200.19 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 244/244 [00:01<00:00, 178.01 examples/s]
01/29/2025 05:59:57 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/27_Nguyá»‡t/parquet_files/27_Nguyá»‡t_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:43<00:00, 12.94s/it]
01/29/2025 06:00:01 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 06:00:01 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.30s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.82s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:20<03:01, 20.15s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:22, 17.78s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:53<02:01, 17.43s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:07<01:38, 16.34s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:25<01:23, 16.80s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:38<01:02, 15.63s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:55<00:47, 15.89s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:09<00:30, 15.32s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:26<00:15, 15.94s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 12.26s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/292 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/292 [00:00<00:06, 43.29 examples/s][A
Postprocessing dataset (num_proc=32):  10%|â–ˆ         | 30/292 [00:00<00:02, 102.06 examples/s][A
Postprocessing dataset (num_proc=32):  20%|â–ˆâ–‰        | 58/292 [00:00<00:01, 151.26 examples/s][A
Postprocessing dataset (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 85/292 [00:00<00:01, 183.47 examples/s][A
Postprocessing dataset (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/292 [00:00<00:00, 224.85 examples/s][A
Postprocessing dataset (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 148/292 [00:00<00:00, 221.48 examples/s][A
Postprocessing dataset (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 175/292 [00:00<00:00, 208.83 examples/s][A
Postprocessing dataset (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 202/292 [00:01<00:00, 211.33 examples/s][A
Postprocessing dataset (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 229/292 [00:01<00:00, 214.62 examples/s][A
Postprocessing dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 265/292 [00:01<00:00, 233.36 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292/292 [00:01<00:00, 198.13 examples/s]
01/29/2025 06:02:44 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/38_PhÆ°Æ¡ng/parquet_files/38_PhÆ°Æ¡ng_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 15.31s/it]
01/29/2025 06:02:48 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 06:02:48 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.45s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.68s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:14<02:12, 14.68s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:27<01:48, 13.55s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:41<01:35, 13.59s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:54<01:21, 13.61s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:12<01:15, 15.15s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:29<01:02, 15.58s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:45<00:47, 15.90s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:58<00:29, 14.99s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:10<00:13, 13.90s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:20<00:00, 12.82s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 50.94 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 151.57 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 187.43 examples/s][A
Postprocessing dataset (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [00:00<00:00, 208.59 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 233.53 examples/s][A
Postprocessing dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 165/300 [00:00<00:00, 205.45 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 232.96 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 229.68 examples/s][A
Postprocessing dataset (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [00:01<00:00, 229.45 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 272.49 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 212.77 examples/s]
01/29/2025 06:05:20 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/16_Ly/parquet_files/16_Ly_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:22<00:00, 14.28s/it]
01/29/2025 06:05:24 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 06:05:24 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.97s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.71s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:14<02:11, 14.60s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:53, 14.18s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:41<01:36, 13.78s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:55<01:21, 13.64s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:09<01:10, 14.05s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:26<01:00, 15.04s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:40<00:43, 14.52s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:53<00:27, 13.95s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:05<00:13, 13.61s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:13<00:00, 11.58s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 50.81 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 153.29 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 196.59 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 213.00 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 221.95 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 228.41 examples/s][A
Postprocessing dataset (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [00:00<00:00, 253.17 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 237.33 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 231.12 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 254.30 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 211.03 examples/s]
01/29/2025 06:07:49 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/14_NhÆ°/parquet_files/14_NhÆ°_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:15<00:00, 13.52s/it]
01/29/2025 06:07:53 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 06:07:53 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.81s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.24s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  2.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.01s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:13<01:36, 13.76s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:26<01:18, 13.10s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:38<01:03, 12.76s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:51<00:50, 12.72s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:04<00:38, 12.79s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:15<00:24, 12.16s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:28<00:12, 12.37s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:38<00:00, 11.72s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/250 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/250 [00:00<00:06, 39.26 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 32/250 [00:00<00:01, 117.73 examples/s][A
Postprocessing dataset (num_proc=32):  22%|â–ˆâ–ˆâ–       | 56/250 [00:00<00:01, 153.16 examples/s][A
Postprocessing dataset (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/250 [00:00<00:00, 197.21 examples/s][A
Postprocessing dataset (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/250 [00:00<00:00, 184.73 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 144/250 [00:00<00:00, 208.62 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 168/250 [00:00<00:00, 188.35 examples/s][A
Postprocessing dataset (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 192/250 [00:01<00:00, 195.53 examples/s][A
Postprocessing dataset (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 215/250 [00:01<00:00, 192.73 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 232.09 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 181.18 examples/s]
01/29/2025 06:09:44 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/4_Huy/parquet_files/4_Huy_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:40<00:00, 12.57s/it]
01/29/2025 06:09:48 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 06:09:48 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.06s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.61s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:14<01:40, 14.41s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:27<01:22, 13.79s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:39<01:04, 12.94s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:53<00:52, 13.22s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:06<00:39, 13.27s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:19<00:26, 13.25s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:33<00:13, 13.24s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:46<00:00, 13.19s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/249 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/249 [00:00<00:06, 39.04 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–Œ        | 40/249 [00:00<00:01, 133.39 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 64/249 [00:00<00:01, 163.00 examples/s][A
Postprocessing dataset (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/249 [00:00<00:00, 180.04 examples/s][A
Postprocessing dataset (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/249 [00:00<00:00, 192.66 examples/s][A
Postprocessing dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 136/249 [00:00<00:00, 200.63 examples/s][A
Postprocessing dataset (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 160/249 [00:00<00:00, 184.11 examples/s][A
Postprocessing dataset (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 192/249 [00:01<00:00, 212.05 examples/s][A
Postprocessing dataset (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 214/249 [00:01<00:00, 210.54 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 242/249 [00:01<00:00, 227.27 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:01<00:00, 184.09 examples/s]
01/29/2025 06:11:45 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/2_Vy/parquet_files/2_Vy_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:48<00:00, 13.55s/it]
01/29/2025 06:11:48 - INFO - __main__ - *** Load annotated dataset ***
01/29/2025 06:11:48 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.78s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.02s/it]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:16<02:25, 16.18s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:56, 14.61s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:42, 14.57s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:26, 14.39s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:09<01:06, 13.40s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:23<00:53, 13.44s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:36<00:39, 13.18s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:50<00:27, 13.61s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:08<00:14, 14.82s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:17<00:00, 13.22s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 52.83 examples/s][A
Postprocessing dataset (num_proc=32):  10%|â–ˆ         | 30/300 [00:00<00:02, 109.23 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 135.51 examples/s][A
Postprocessing dataset (num_proc=32):  30%|â–ˆâ–ˆâ–ˆ       | 90/300 [00:00<00:01, 209.69 examples/s][A
Postprocessing dataset (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 119/300 [00:00<00:00, 219.00 examples/s][A
Postprocessing dataset (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 156/300 [00:00<00:00, 246.99 examples/s][A
Postprocessing dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 183/300 [00:00<00:00, 242.20 examples/s][A
Postprocessing dataset (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [00:01<00:00, 238.21 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 236.90 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 232.38 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 213.66 examples/s]
01/29/2025 06:14:19 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/45_Nhi/parquet_files/45_Nhi_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:19<00:00, 14.00s/it]
01/29/2025 06:14:23 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 31374.92 examples/s]
01/29/2025 06:14:23 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.88s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.57s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 47.86 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 139.28 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 187.33 examples/s]Preparing prompts (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [00:00<00:00, 210.06 examples/s]Preparing prompts (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 129/300 [00:00<00:00, 225.51 examples/s]Preparing prompts (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 156/300 [00:00<00:00, 222.43 examples/s]Preparing prompts (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 192/300 [00:00<00:00, 252.91 examples/s]Preparing prompts (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 219/300 [00:01<00:00, 240.69 examples/s]Preparing prompts (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 246/300 [00:01<00:00, 233.96 examples/s]Preparing prompts (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [00:01<00:00, 265.57 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 220.68 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:12<01:50, 12.33s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:24<01:37, 12.15s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:35<01:22, 11.72s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:46<01:09, 11.57s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:57<00:56, 11.23s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:07<00:43, 10.83s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:18<00:32, 10.90s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:30<00:22, 11.36s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:41<00:11, 11.13s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:47<00:00,  9.53s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 52.82 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:02, 126.20 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 202.45 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 230.40 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 258.33 examples/s][A
Postprocessing dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 183/300 [00:00<00:00, 228.87 examples/s][A
Postprocessing dataset (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 219/300 [00:01<00:00, 222.24 examples/s][A
Postprocessing dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 246/300 [00:01<00:00, 223.00 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 275.72 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 217.61 examples/s]
01/29/2025 06:16:24 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/21_Huy/parquet_files/21_Huy_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:49<00:00, 10.97s/it]
01/29/2025 06:16:26 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 99046.85 examples/s]
01/29/2025 06:16:26 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.96s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.85s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 44.58 examples/s]Preparing prompts (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 178.48 examples/s]Preparing prompts (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:00, 220.18 examples/s]Preparing prompts (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 197.32 examples/s]Preparing prompts (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 195.80 examples/s]Preparing prompts (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 214.85 examples/s]Preparing prompts (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [00:00<00:00, 248.70 examples/s]Preparing prompts (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 246/300 [00:01<00:00, 261.93 examples/s]Preparing prompts (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [00:01<00:00, 267.67 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 222.04 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:14<02:10, 14.47s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:26<01:46, 13.27s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:41<01:37, 13.88s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:53<01:17, 12.97s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:11<01:13, 14.78s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:24<00:56, 14.18s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:38<00:42, 14.18s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:50<00:27, 13.67s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:05<00:14, 14.01s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:12<00:00, 11.71s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 51.58 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 145.46 examples/s][A
Postprocessing dataset (num_proc=32):  20%|â–ˆâ–ˆ        | 60/300 [00:00<00:01, 159.88 examples/s][A
Postprocessing dataset (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [00:00<00:00, 223.94 examples/s][A
Postprocessing dataset (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 129/300 [00:00<00:00, 235.91 examples/s][A
Postprocessing dataset (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 156/300 [00:00<00:00, 233.21 examples/s][A
Postprocessing dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 183/300 [00:00<00:00, 230.92 examples/s][A
Postprocessing dataset (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 219/300 [00:01<00:00, 253.00 examples/s][A
Postprocessing dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 246/300 [00:01<00:00, 207.18 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [00:01<00:00, 236.39 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 209.05 examples/s]
01/29/2025 06:18:54 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/43_NhÃ£/parquet_files/43_NhÃ£_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:14<00:00, 13.44s/it]
01/29/2025 06:18:58 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 247 examples [00:00, 77284.08 examples/s]
01/29/2025 06:18:58 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.86s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.63s/it]
Preparing prompts (num_proc=32):   0%|          | 0/247 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/247 [00:00<00:06, 34.55 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 32/247 [00:00<00:02, 102.99 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 56/247 [00:00<00:01, 133.97 examples/s]Preparing prompts (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/247 [00:00<00:00, 188.03 examples/s]Preparing prompts (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 112/247 [00:00<00:00, 195.70 examples/s]Preparing prompts (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 136/247 [00:00<00:00, 191.74 examples/s]Preparing prompts (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 160/247 [00:00<00:00, 199.66 examples/s]Preparing prompts (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 191/247 [00:01<00:00, 217.58 examples/s]Preparing prompts (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 219/247 [00:01<00:00, 222.26 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:01<00:00, 234.96 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:01<00:00, 181.61 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:12<01:26, 12.31s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:23<01:10, 11.80s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:37<01:02, 12.58s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:55<00:59, 14.80s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:08<00:42, 14.01s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:23<00:28, 14.43s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:35<00:13, 13.70s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:46<00:00, 12.77s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/247 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/247 [00:00<00:06, 39.10 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–Œ        | 40/247 [00:00<00:01, 132.99 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 64/247 [00:00<00:01, 164.42 examples/s][A
Postprocessing dataset (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/247 [00:00<00:00, 178.08 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 120/247 [00:00<00:00, 193.31 examples/s][A
Postprocessing dataset (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 152/247 [00:00<00:00, 213.80 examples/s][A
Postprocessing dataset (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 176/247 [00:00<00:00, 219.72 examples/s][A
Postprocessing dataset (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 205/247 [00:01<00:00, 194.34 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 233/247 [00:01<00:00, 213.13 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:01<00:00, 182.89 examples/s]
01/29/2025 06:20:58 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/10_HÆ°á»ng/parquet_files/10_HÆ°á»ng_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:48<00:00, 13.55s/it]
01/29/2025 06:21:01 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 299 examples [00:00, 96976.25 examples/s]
01/29/2025 06:21:01 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.99s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.24s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.10s/it]
Preparing prompts (num_proc=32):   0%|          | 0/299 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/299 [00:00<00:06, 44.56 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 40/299 [00:00<00:02, 128.44 examples/s]Preparing prompts (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/299 [00:00<00:01, 198.03 examples/s]Preparing prompts (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 109/299 [00:00<00:00, 218.05 examples/s]Preparing prompts (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 137/299 [00:00<00:00, 222.00 examples/s]Preparing prompts (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 164/299 [00:00<00:00, 210.40 examples/s]Preparing prompts (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 191/299 [00:00<00:00, 223.62 examples/s]Preparing prompts (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 218/299 [00:01<00:00, 235.42 examples/s]Preparing prompts (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 245/299 [00:01<00:00, 231.98 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 308.08 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 218.85 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:15<02:21, 15.70s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:55, 14.46s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:40, 14.40s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:56<01:23, 13.91s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:10<01:10, 14.00s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:27<00:58, 14.71s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:39<00:41, 13.82s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:51<00:26, 13.43s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:04<00:13, 13.39s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:10<00:00, 10.93s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/299 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/299 [00:00<00:05, 48.51 examples/s][A
Postprocessing dataset (num_proc=32):  10%|â–ˆ         | 30/299 [00:00<00:02, 95.00 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/299 [00:00<00:01, 175.78 examples/s][A
Postprocessing dataset (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/299 [00:00<00:01, 179.92 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 146/299 [00:00<00:00, 242.67 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 173/299 [00:00<00:00, 241.93 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 200/299 [00:00<00:00, 246.92 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 227/299 [00:01<00:00, 241.95 examples/s][A
Postprocessing dataset (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 254/299 [00:01<00:00, 230.73 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 274.99 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 211.08 examples/s]
01/29/2025 06:23:26 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/26_Háº£i/parquet_files/26_Háº£i_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:12<00:00, 13.25s/it]
01/29/2025 06:23:30 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 131 examples [00:00, 44118.66 examples/s]
01/29/2025 06:23:30 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.39s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.81s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.79s/it]
Preparing prompts (num_proc=32):   0%|          | 0/131 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   4%|â–         | 5/131 [00:00<00:05, 23.58 examples/s]Preparing prompts (num_proc=32):  18%|â–ˆâ–Š        | 23/131 [00:00<00:01, 85.03 examples/s]Preparing prompts (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 35/131 [00:00<00:01, 80.32 examples/s]Preparing prompts (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 51/131 [00:00<00:00, 98.90 examples/s]Preparing prompts (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 63/131 [00:00<00:00, 98.53 examples/s]Preparing prompts (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 75/131 [00:00<00:00, 95.82 examples/s]Preparing prompts (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 87/131 [00:01<00:00, 86.88 examples/s]Preparing prompts (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 99/131 [00:01<00:00, 94.45 examples/s]Preparing prompts (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 123/131 [00:01<00:00, 131.55 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:01<00:00, 98.44 examples/s] 
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/5 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  20%|â–ˆâ–ˆ        | 1/5 [00:12<00:50, 12.72s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:24<00:35, 11.95s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:36<00:24, 12.10s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:47<00:11, 11.60s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:50<00:00,  8.65s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/131 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   4%|â–         | 5/131 [00:00<00:06, 20.56 examples/s][A
Postprocessing dataset (num_proc=32):  18%|â–ˆâ–Š        | 23/131 [00:00<00:01, 72.51 examples/s][A
Postprocessing dataset (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 39/131 [00:00<00:01, 90.75 examples/s][A
Postprocessing dataset (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 55/131 [00:00<00:00, 102.93 examples/s][A
Postprocessing dataset (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 71/131 [00:00<00:00, 107.96 examples/s][A
Postprocessing dataset (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 87/131 [00:00<00:00, 109.07 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 103/131 [00:01<00:00, 117.22 examples/s][A
Postprocessing dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 119/131 [00:01<00:00, 119.91 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:01<00:00, 100.21 examples/s]
01/29/2025 06:24:34 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/37_Háº­u/parquet_files/37_Háº­u_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:52<00:00, 10.55s/it]
01/29/2025 06:24:38 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 245 examples [00:00, 79659.26 examples/s]
01/29/2025 06:24:38 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.53s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.81s/it]
Preparing prompts (num_proc=32):   0%|          | 0/245 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/245 [00:00<00:05, 41.35 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 32/245 [00:00<00:01, 107.72 examples/s]Preparing prompts (num_proc=32):  20%|â–ˆâ–‰        | 48/245 [00:00<00:01, 124.83 examples/s]Preparing prompts (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 72/245 [00:00<00:01, 155.65 examples/s]Preparing prompts (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 96/245 [00:00<00:00, 175.95 examples/s]Preparing prompts (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 120/245 [00:00<00:00, 188.27 examples/s]Preparing prompts (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 152/245 [00:00<00:00, 221.43 examples/s]Preparing prompts (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 175/245 [00:01<00:00, 181.52 examples/s]Preparing prompts (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 203/245 [00:01<00:00, 204.66 examples/s]Preparing prompts (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 238/245 [00:01<00:00, 242.07 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245/245 [00:01<00:00, 181.36 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:13<01:34, 13.51s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:27<01:21, 13.60s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:40<01:08, 13.65s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:52<00:51, 12.80s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:13<00:47, 15.93s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:27<00:30, 15.31s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:41<00:14, 14.63s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:52<00:00, 13.53s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/245 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/245 [00:00<00:05, 41.31 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 32/245 [00:00<00:01, 109.98 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 56/245 [00:00<00:01, 147.30 examples/s][A
Postprocessing dataset (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/245 [00:00<00:00, 177.87 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 112/245 [00:00<00:00, 190.84 examples/s][A
Postprocessing dataset (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 144/245 [00:00<00:00, 203.04 examples/s][A
Postprocessing dataset (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 168/245 [00:00<00:00, 195.43 examples/s][A
Postprocessing dataset (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 203/245 [00:01<00:00, 207.96 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 238/245 [00:01<00:00, 230.25 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245/245 [00:01<00:00, 181.23 examples/s]
01/29/2025 06:26:44 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/7_Minh/parquet_files/7_Minh_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:54<00:00, 14.31s/it]
01/29/2025 06:26:48 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 261 examples [00:00, 83860.38 examples/s]
01/29/2025 06:26:48 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.43s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.85s/it]
Preparing prompts (num_proc=32):   0%|          | 0/261 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 9/261 [00:00<00:05, 43.85 examples/s]Preparing prompts (num_proc=32):  17%|â–ˆâ–‹        | 45/261 [00:00<00:01, 124.67 examples/s]Preparing prompts (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 85/261 [00:00<00:00, 187.00 examples/s]Preparing prompts (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 109/261 [00:00<00:00, 159.19 examples/s]Preparing prompts (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 157/261 [00:00<00:00, 230.23 examples/s]Preparing prompts (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 189/261 [00:01<00:00, 201.75 examples/s]Preparing prompts (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 213/261 [00:01<00:00, 202.37 examples/s]Preparing prompts (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 253/261 [00:01<00:00, 248.05 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261/261 [00:01<00:00, 193.93 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/9 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  11%|â–ˆ         | 1/9 [00:12<01:41, 12.66s/it] ... :  22%|â–ˆâ–ˆâ–       | 2/9 [00:23<01:22, 11.80s/it] ... :  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:38<01:18, 13.11s/it] ... :  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:53<01:08, 13.73s/it] ... :  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [01:04<00:51, 12.87s/it] ... :  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [01:15<00:36, 12.16s/it] ... :  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [01:26<00:23, 11.99s/it] ... :  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:40<00:12, 12.62s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:45<00:00, 10.16s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/261 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 9/261 [00:00<00:05, 45.86 examples/s][A
Postprocessing dataset (num_proc=32):  10%|â–ˆ         | 27/261 [00:00<00:02, 101.57 examples/s][A
Postprocessing dataset (num_proc=32):  20%|â–ˆâ–ˆ        | 53/261 [00:00<00:01, 150.76 examples/s][A
Postprocessing dataset (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 93/261 [00:00<00:00, 196.89 examples/s][A
Postprocessing dataset (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 125/261 [00:00<00:00, 205.82 examples/s][A
Postprocessing dataset (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 149/261 [00:00<00:00, 212.66 examples/s][A
Postprocessing dataset (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 173/261 [00:00<00:00, 196.43 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 205/261 [00:01<00:00, 186.50 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 253/261 [00:01<00:00, 250.71 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261/261 [00:01<00:00, 193.42 examples/s]
01/29/2025 06:28:48 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/35_Há»“/parquet_files/35_Há»“_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:47<00:00, 11.99s/it]
01/29/2025 06:28:52 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 110 examples [00:00, 37963.75 examples/s]
01/29/2025 06:28:52 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.35s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.83s/it]
Preparing prompts (num_proc=32):   0%|          | 0/110 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   4%|â–Ž         | 4/110 [00:00<00:04, 21.30 examples/s]Preparing prompts (num_proc=32):  18%|â–ˆâ–Š        | 20/110 [00:00<00:01, 68.95 examples/s]Preparing prompts (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 32/110 [00:00<00:01, 72.12 examples/s]Preparing prompts (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 48/110 [00:00<00:00, 90.06 examples/s]Preparing prompts (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 62/110 [00:00<00:00, 84.71 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 74/110 [00:00<00:00, 91.00 examples/s]Preparing prompts (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 86/110 [00:01<00:00, 79.44 examples/s]Preparing prompts (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 101/110 [00:01<00:00, 90.79 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [00:01<00:00, 80.66 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/4 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:12<00:37, 12.63s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:24<00:24, 12.26s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:37<00:12, 12.70s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:44<00:00, 10.42s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/110 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   4%|â–Ž         | 4/110 [00:00<00:04, 22.28 examples/s][A
Postprocessing dataset (num_proc=32):  18%|â–ˆâ–Š        | 20/110 [00:00<00:01, 74.49 examples/s][A
Postprocessing dataset (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 32/110 [00:00<00:00, 78.10 examples/s][A
Postprocessing dataset (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 48/110 [00:00<00:00, 97.75 examples/s][A
Postprocessing dataset (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 65/110 [00:00<00:00, 110.12 examples/s][A
Postprocessing dataset (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 77/110 [00:00<00:00, 98.59 examples/s] [A
Postprocessing dataset (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 89/110 [00:01<00:00, 90.10 examples/s][A
Postprocessing dataset (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 101/110 [00:01<00:00, 96.98 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [00:01<00:00, 87.38 examples/s]
01/29/2025 06:29:50 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/23_Tuáº¥n/parquet_files/23_Tuáº¥n_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:46<00:00, 11.68s/it]
01/29/2025 06:29:54 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 95672.99 examples/s]
01/29/2025 06:29:54 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.31s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.58s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.51s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 49.36 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 136.65 examples/s]Preparing prompts (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 191.67 examples/s]Preparing prompts (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 199.07 examples/s]Preparing prompts (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 226.54 examples/s]Preparing prompts (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 192/300 [00:00<00:00, 235.83 examples/s]Preparing prompts (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 219/300 [00:01<00:00, 221.40 examples/s]Preparing prompts (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [00:01<00:00, 243.50 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 222.65 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:16<02:27, 16.44s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:49, 13.66s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:41<01:33, 13.43s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:55<01:22, 13.72s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:08<01:07, 13.47s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:23<00:56, 14.11s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:38<00:43, 14.43s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:53<00:29, 14.58s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:08<00:14, 14.64s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:14<00:00, 12.05s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 48.79 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 179.24 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/300 [00:00<00:01, 193.41 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 215.21 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [00:00<00:00, 222.70 examples/s][A
Postprocessing dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 165/300 [00:00<00:00, 214.05 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 228.18 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 249.99 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 232.66 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 258.69 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 212.14 examples/s]
01/29/2025 06:32:22 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/17_Hiá»n/parquet_files/17_Hiá»n_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:17<00:00, 13.71s/it]
01/29/2025 06:32:26 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 100 examples [00:00, 35148.78 examples/s]
01/29/2025 06:32:26 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.86s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.48s/it]
Preparing prompts (num_proc=32):   0%|          | 0/100 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   4%|â–         | 4/100 [00:00<00:04, 23.33 examples/s]Preparing prompts (num_proc=32):  12%|â–ˆâ–        | 12/100 [00:00<00:01, 48.15 examples/s]Preparing prompts (num_proc=32):  19%|â–ˆâ–‰        | 19/100 [00:00<00:01, 56.28 examples/s]Preparing prompts (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:00<00:01, 66.96 examples/s]Preparing prompts (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:00<00:00, 66.02 examples/s]Preparing prompts (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:00<00:00, 72.53 examples/s]Preparing prompts (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:00<00:00, 82.37 examples/s]Preparing prompts (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:01<00:00, 82.85 examples/s]Preparing prompts (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:01<00:00, 77.69 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 75.72 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/4 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:10<00:32, 10.67s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:20<00:20, 10.18s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:32<00:11, 11.20s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:35<00:00,  7.95s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/100 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   4%|â–         | 4/100 [00:00<00:05, 18.93 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–Œ        | 16/100 [00:00<00:01, 57.03 examples/s][A
Postprocessing dataset (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:00<00:00, 72.36 examples/s][A
Postprocessing dataset (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:00<00:00, 83.24 examples/s][A
Postprocessing dataset (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:00<00:00, 85.68 examples/s][A
Postprocessing dataset (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:00<00:00, 78.30 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:00<00:00, 90.95 examples/s][A
Postprocessing dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:01<00:00, 86.61 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 78.03 examples/s]
01/29/2025 06:33:15 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/24_Long/parquet_files/24_Long_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:37<00:00,  9.48s/it]
01/29/2025 06:33:18 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 247 examples [00:00, 80678.54 examples/s]
01/29/2025 06:33:18 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.72s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.35s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.62s/it]
Preparing prompts (num_proc=32):   0%|          | 0/247 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/247 [00:00<00:05, 40.01 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 32/247 [00:00<00:01, 108.41 examples/s]Preparing prompts (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 64/247 [00:00<00:01, 169.77 examples/s]Preparing prompts (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/247 [00:00<00:00, 187.65 examples/s]Preparing prompts (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 112/247 [00:00<00:00, 184.14 examples/s]Preparing prompts (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 136/247 [00:00<00:00, 178.81 examples/s]Preparing prompts (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 160/247 [00:00<00:00, 175.75 examples/s]Preparing prompts (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 184/247 [00:01<00:00, 187.19 examples/s]Preparing prompts (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 219/247 [00:01<00:00, 222.35 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:01<00:00, 186.07 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:16<01:57, 16.85s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:31<01:34, 15.71s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:45<01:13, 14.75s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [01:01<01:00, 15.12s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:14<00:43, 14.63s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:28<00:28, 14.24s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:42<00:14, 14.32s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:54<00:00, 13.51s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/247 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/247 [00:00<00:05, 43.20 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 32/247 [00:00<00:02, 95.08 examples/s][A
Postprocessing dataset (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 72/247 [00:00<00:01, 168.53 examples/s][A
Postprocessing dataset (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 104/247 [00:00<00:00, 179.65 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 144/247 [00:00<00:00, 214.98 examples/s][A
Postprocessing dataset (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 168/247 [00:00<00:00, 208.13 examples/s][A
Postprocessing dataset (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 198/247 [00:01<00:00, 208.35 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 233/247 [00:01<00:00, 215.38 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:01<00:00, 183.59 examples/s]
01/29/2025 06:35:26 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/1_Lan/parquet_files/1_Lan_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:56<00:00, 14.58s/it]
01/29/2025 06:35:30 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 295 examples [00:00, 95752.95 examples/s]
01/29/2025 06:35:30 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.47s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.81s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.50s/it]
Preparing prompts (num_proc=32):   0%|          | 0/295 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/295 [00:00<00:05, 51.01 examples/s]Preparing prompts (num_proc=32):  10%|â–ˆ         | 30/295 [00:00<00:02, 98.31 examples/s]Preparing prompts (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 79/295 [00:00<00:01, 182.18 examples/s]Preparing prompts (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/295 [00:00<00:00, 216.24 examples/s]Preparing prompts (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 151/295 [00:00<00:00, 184.80 examples/s]Preparing prompts (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 205/295 [00:00<00:00, 260.30 examples/s]Preparing prompts (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 241/295 [00:01<00:00, 229.28 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [00:01<00:00, 283.48 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [00:01<00:00, 214.65 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:13<02:04, 13.87s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:25<01:40, 12.54s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:38<01:28, 12.58s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:49<01:13, 12.25s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:05<01:06, 13.39s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:18<00:53, 13.32s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:31<00:39, 13.29s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:44<00:26, 13.12s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:58<00:13, 13.47s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:04<00:00, 11.17s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/295 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/295 [00:00<00:05, 49.70 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/295 [00:00<00:01, 146.08 examples/s][A
Postprocessing dataset (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 88/295 [00:00<00:01, 197.23 examples/s][A
Postprocessing dataset (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 115/295 [00:00<00:00, 204.95 examples/s][A
Postprocessing dataset (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 151/295 [00:00<00:00, 221.35 examples/s][A
Postprocessing dataset (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 187/295 [00:00<00:00, 245.83 examples/s][A
Postprocessing dataset (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 214/295 [00:01<00:00, 222.25 examples/s][A
Postprocessing dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 241/295 [00:01<00:00, 221.98 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 277/295 [00:01<00:00, 250.86 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [00:01<00:00, 210.76 examples/s]
01/29/2025 06:37:47 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/22_Quyáº¿t/parquet_files/22_Quyáº¿t_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:06<00:00, 12.68s/it]
01/29/2025 06:37:51 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 248 examples [00:00, 86740.11 examples/s]
01/29/2025 06:37:51 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.40s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.82s/it]
Preparing prompts (num_proc=32):   0%|          | 0/248 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/248 [00:00<00:06, 36.44 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 32/248 [00:00<00:01, 112.34 examples/s]Preparing prompts (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 64/248 [00:00<00:01, 170.55 examples/s]Preparing prompts (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/248 [00:00<00:01, 154.90 examples/s]Preparing prompts (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 112/248 [00:00<00:00, 162.12 examples/s]Preparing prompts (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 144/248 [00:00<00:00, 202.50 examples/s]Preparing prompts (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 168/248 [00:00<00:00, 200.37 examples/s]Preparing prompts (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 191/248 [00:01<00:00, 193.66 examples/s]Preparing prompts (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 220/248 [00:01<00:00, 218.36 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [00:01<00:00, 182.52 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:14<01:40, 14.39s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:28<01:26, 14.47s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:44<01:15, 15.09s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:58<00:59, 14.75s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:14<00:44, 14.99s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:28<00:29, 14.68s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:42<00:14, 14.49s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:55<00:00, 13.90s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/248 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/248 [00:00<00:06, 39.53 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–Œ        | 40/248 [00:00<00:01, 131.67 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 64/248 [00:00<00:01, 155.41 examples/s][A
Postprocessing dataset (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 96/248 [00:00<00:00, 176.79 examples/s][A
Postprocessing dataset (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 128/248 [00:00<00:00, 208.85 examples/s][A
Postprocessing dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 152/248 [00:00<00:00, 195.78 examples/s][A
Postprocessing dataset (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 176/248 [00:00<00:00, 201.49 examples/s][A
Postprocessing dataset (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 199/248 [00:01<00:00, 197.47 examples/s][A
Postprocessing dataset (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 227/248 [00:01<00:00, 214.84 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [00:01<00:00, 181.26 examples/s]
01/29/2025 06:40:01 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/5_Nhi/parquet_files/5_Nhi_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:57<00:00, 14.67s/it]
01/29/2025 06:40:04 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 170 examples [00:00, 55963.56 examples/s]
01/29/2025 06:40:04 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.14s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.58s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.55s/it]
Preparing prompts (num_proc=32):   0%|          | 0/170 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   4%|â–Ž         | 6/170 [00:00<00:05, 29.38 examples/s]Preparing prompts (num_proc=32):  14%|â–ˆâ–        | 24/170 [00:00<00:01, 86.44 examples/s]Preparing prompts (num_proc=32):  25%|â–ˆâ–ˆâ–       | 42/170 [00:00<00:01, 103.57 examples/s]Preparing prompts (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 60/170 [00:00<00:00, 126.50 examples/s]Preparing prompts (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 75/170 [00:00<00:00, 132.73 examples/s]Preparing prompts (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 90/170 [00:00<00:00, 134.18 examples/s]Preparing prompts (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 105/170 [00:00<00:00, 130.01 examples/s]Preparing prompts (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 120/170 [00:01<00:00, 124.10 examples/s]Preparing prompts (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 140/170 [00:01<00:00, 143.57 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:01<00:00, 181.01 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:01<00:00, 129.45 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/6 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  17%|â–ˆâ–‹        | 1/6 [00:11<00:58, 11.61s/it] ... :  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:22<00:45, 11.41s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:34<00:33, 11.28s/it] ... :  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:44<00:22, 11.03s/it] ... :  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:55<00:10, 10.85s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:59<00:00,  8.74s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/170 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   4%|â–Ž         | 6/170 [00:00<00:04, 33.37 examples/s][A
Postprocessing dataset (num_proc=32):  14%|â–ˆâ–        | 24/170 [00:00<00:01, 93.51 examples/s][A
Postprocessing dataset (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 48/170 [00:00<00:00, 134.93 examples/s][A
Postprocessing dataset (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 65/170 [00:00<00:00, 141.91 examples/s][A
Postprocessing dataset (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 80/170 [00:00<00:00, 143.00 examples/s][A
Postprocessing dataset (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 100/170 [00:00<00:00, 142.26 examples/s][A
Postprocessing dataset (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 120/170 [00:00<00:00, 153.72 examples/s][A
Postprocessing dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 140/170 [00:01<00:00, 149.49 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 160/170 [00:01<00:00, 153.35 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:01<00:00, 132.84 examples/s]
01/29/2025 06:41:17 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/28_Nghá»‹/parquet_files/28_Nghá»‹_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:01<00:00, 10.32s/it]
01/29/2025 06:41:21 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 102192.09 examples/s]
01/29/2025 06:41:21 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.51s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.78s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 44.07 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 135.00 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 183.88 examples/s]Preparing prompts (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 225.14 examples/s]Preparing prompts (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 253.74 examples/s]Preparing prompts (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 183/300 [00:00<00:00, 235.34 examples/s]Preparing prompts (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [00:01<00:00, 227.30 examples/s]Preparing prompts (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 246/300 [00:01<00:00, 245.69 examples/s]Preparing prompts (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 273/300 [00:01<00:00, 238.86 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 220.65 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:14<02:12, 14.75s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:28<01:53, 14.18s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:43<01:41, 14.53s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:00<01:34, 15.68s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:20, 16.06s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:35<01:06, 16.59s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:50<00:48, 16.29s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:08<00:33, 16.62s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:22<00:16, 16.01s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:31<00:00, 13.65s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:06, 48.21 examples/s][A
Postprocessing dataset (num_proc=32):  17%|â–ˆâ–‹        | 50/300 [00:00<00:01, 166.68 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 164.66 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:00<00:00, 227.57 examples/s][A
Postprocessing dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [00:00<00:00, 253.83 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 249.55 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 244.27 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 224.19 examples/s][A
Postprocessing dataset (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [00:01<00:00, 228.92 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 291/300 [00:01<00:00, 262.52 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 217.29 examples/s]
01/29/2025 06:44:06 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/42_Äoan/parquet_files/42_Äoan_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:33<00:00, 15.34s/it]
01/29/2025 06:44:10 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 90563.64 examples/s]
01/29/2025 06:44:10 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.41s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.66s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:04, 58.04 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 144.76 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 187.62 examples/s]Preparing prompts (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [00:00<00:00, 207.74 examples/s]Preparing prompts (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 129/300 [00:00<00:00, 193.97 examples/s]Preparing prompts (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 165/300 [00:00<00:00, 230.55 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 231.34 examples/s]Preparing prompts (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 230.38 examples/s]Preparing prompts (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 245.07 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 218.34 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:15<02:21, 15.73s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:31<02:04, 15.60s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:41, 14.50s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:58<01:25, 14.21s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:14<01:14, 14.98s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:26<00:55, 13.92s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:41<00:43, 14.44s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:55<00:28, 14.04s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:09<00:14, 14.00s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:16<00:00, 11.95s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 49.15 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 145.22 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 195.58 examples/s][A
Postprocessing dataset (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [00:00<00:00, 203.43 examples/s][A
Postprocessing dataset (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 129/300 [00:00<00:00, 208.82 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 247.19 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [00:00<00:00, 240.60 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [00:01<00:00, 243.74 examples/s][A
Postprocessing dataset (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [00:01<00:00, 225.06 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 268.82 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 214.71 examples/s]
01/29/2025 06:46:40 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/15_Äá»©c/parquet_files/15_Äá»©c_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:18<00:00, 13.85s/it]
01/29/2025 06:46:44 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 299 examples [00:00, 101901.10 examples/s]
01/29/2025 06:46:44 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.01s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.80s/it]
Preparing prompts (num_proc=32):   0%|          | 0/299 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/299 [00:00<00:06, 47.13 examples/s]Preparing prompts (num_proc=32):  17%|â–ˆâ–‹        | 50/299 [00:00<00:01, 152.91 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/299 [00:00<00:01, 163.07 examples/s]Preparing prompts (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/299 [00:00<00:01, 190.33 examples/s]Preparing prompts (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 128/299 [00:00<00:00, 190.73 examples/s]Preparing prompts (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 173/299 [00:00<00:00, 258.23 examples/s]Preparing prompts (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 209/299 [00:01<00:00, 244.84 examples/s]Preparing prompts (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 236/299 [00:01<00:00, 224.61 examples/s]Preparing prompts (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 290/299 [00:01<00:00, 298.35 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 220.35 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:13<02:00, 13.34s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:25<01:40, 12.57s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:39<01:32, 13.24s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:52<01:17, 12.99s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:05<01:05, 13.17s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:19<00:53, 13.42s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:32<00:40, 13.44s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:45<00:26, 13.05s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:59<00:13, 13.45s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:05<00:00, 11.26s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/299 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/299 [00:00<00:06, 46.41 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/299 [00:00<00:02, 111.55 examples/s][A
Postprocessing dataset (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 80/299 [00:00<00:01, 196.55 examples/s][A
Postprocessing dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/299 [00:00<00:00, 201.90 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 137/299 [00:00<00:00, 212.72 examples/s][A
Postprocessing dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 164/299 [00:00<00:00, 216.45 examples/s][A
Postprocessing dataset (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 191/299 [00:01<00:00, 206.32 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 227/299 [00:01<00:00, 225.76 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 263/299 [00:01<00:00, 253.51 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 267.63 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:01<00:00, 205.37 examples/s]
01/29/2025 06:49:03 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/19_Nam/parquet_files/19_Nam_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:08<00:00, 12.80s/it]
01/29/2025 06:49:07 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 298 examples [00:00, 101585.06 examples/s]
01/29/2025 06:49:07 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.68s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.70s/it]
Preparing prompts (num_proc=32):   0%|          | 0/298 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/298 [00:00<00:06, 45.18 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 40/298 [00:00<00:01, 129.70 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/298 [00:00<00:01, 172.28 examples/s]Preparing prompts (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 109/298 [00:00<00:00, 216.26 examples/s]Preparing prompts (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 145/298 [00:00<00:00, 246.57 examples/s]Preparing prompts (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 172/298 [00:00<00:00, 232.93 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 199/298 [00:00<00:00, 226.76 examples/s]Preparing prompts (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 235/298 [00:01<00:00, 234.81 examples/s]Preparing prompts (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 271/298 [00:01<00:00, 266.11 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 298/298 [00:01<00:00, 219.18 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:14<02:11, 14.60s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:26<01:42, 12.81s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:36<01:22, 11.85s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:49<01:13, 12.19s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:01<01:00, 12.17s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:13<00:48, 12.16s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:25<00:35, 11.97s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:36<00:23, 11.66s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:51<00:12, 12.63s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:56<00:00, 10.40s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/298 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/298 [00:00<00:05, 50.44 examples/s][A
Postprocessing dataset (num_proc=32):  10%|â–ˆ         | 30/298 [00:00<00:02, 101.21 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/298 [00:00<00:01, 202.13 examples/s][A
Postprocessing dataset (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/298 [00:00<00:00, 221.81 examples/s][A
Postprocessing dataset (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 127/298 [00:00<00:00, 205.16 examples/s][A
Postprocessing dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 163/298 [00:00<00:00, 246.82 examples/s][A
Postprocessing dataset (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 190/298 [00:00<00:00, 224.18 examples/s][A
Postprocessing dataset (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 217/298 [00:01<00:00, 236.11 examples/s][A
Postprocessing dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 244/298 [00:01<00:00, 244.08 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 280/298 [00:01<00:00, 259.87 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 298/298 [00:01<00:00, 214.62 examples/s]
01/29/2025 06:51:17 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/18_Máº¡nh/parquet_files/18_Máº¡nh_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:58<00:00, 11.88s/it]
01/29/2025 06:51:21 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 220 examples [00:00, 72309.92 examples/s]
01/29/2025 06:51:21 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.83s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.64s/it]
Preparing prompts (num_proc=32):   0%|          | 0/220 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 7/220 [00:00<00:06, 32.78 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 28/220 [00:00<00:02, 92.52 examples/s]Preparing prompts (num_proc=32):  29%|â–ˆâ–ˆâ–Š       | 63/220 [00:00<00:01, 152.93 examples/s]Preparing prompts (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 84/220 [00:00<00:00, 146.77 examples/s]Preparing prompts (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 112/220 [00:00<00:00, 152.15 examples/s]Preparing prompts (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 140/220 [00:00<00:00, 179.15 examples/s]Preparing prompts (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 168/220 [00:01<00:00, 162.85 examples/s]Preparing prompts (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 214/220 [00:01<00:00, 216.64 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:01<00:00, 162.87 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/7 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  14%|â–ˆâ–        | 1/7 [00:13<01:22, 13.80s/it] ... :  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:27<01:10, 14.01s/it] ... :  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:42<00:56, 14.17s/it] ... :  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:58<00:44, 14.81s/it] ... :  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [01:11<00:28, 14.12s/it] ... :  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [01:26<00:14, 14.44s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:37<00:00, 13.44s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/220 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 7/220 [00:00<00:05, 36.72 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–Œ        | 35/220 [00:00<00:01, 130.02 examples/s][A
Postprocessing dataset (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 56/220 [00:00<00:01, 140.81 examples/s][A
Postprocessing dataset (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 77/220 [00:00<00:00, 151.89 examples/s][A
Postprocessing dataset (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 98/220 [00:00<00:00, 161.13 examples/s][A
Postprocessing dataset (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 126/220 [00:00<00:00, 181.56 examples/s][A
Postprocessing dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 147/220 [00:00<00:00, 188.38 examples/s][A
Postprocessing dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 168/220 [00:01<00:00, 184.29 examples/s][A
Postprocessing dataset (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 189/220 [00:01<00:00, 181.23 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:01<00:00, 209.72 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:01<00:00, 164.77 examples/s]
01/29/2025 06:53:13 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/33_HoÃ ng/parquet_files/33_HoÃ ng_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:39<00:00, 14.21s/it]
01/29/2025 06:53:17 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 248 examples [00:00, 85373.23 examples/s]
01/29/2025 06:53:17 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.85s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.75s/it]
Preparing prompts (num_proc=32):   0%|          | 0/248 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/248 [00:00<00:05, 41.45 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 32/248 [00:00<00:01, 115.85 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 56/248 [00:00<00:01, 141.17 examples/s]Preparing prompts (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 72/248 [00:00<00:01, 137.93 examples/s]Preparing prompts (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 96/248 [00:00<00:01, 149.77 examples/s]Preparing prompts (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 144/248 [00:00<00:00, 211.58 examples/s]Preparing prompts (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 168/248 [00:00<00:00, 206.45 examples/s]Preparing prompts (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 191/248 [00:01<00:00, 190.95 examples/s]Preparing prompts (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 227/248 [00:01<00:00, 228.86 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [00:01<00:00, 182.56 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:14<01:44, 14.95s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:31<01:35, 15.90s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:46<01:17, 15.54s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [01:01<01:01, 15.31s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:16<00:45, 15.24s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:32<00:31, 15.60s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:46<00:14, 14.96s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:58<00:00, 13.95s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/248 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/248 [00:00<00:06, 39.21 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 32/248 [00:00<00:01, 118.57 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 56/248 [00:00<00:01, 154.75 examples/s][A
Postprocessing dataset (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 80/248 [00:00<00:01, 161.69 examples/s][A
Postprocessing dataset (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 104/248 [00:00<00:00, 181.64 examples/s][A
Postprocessing dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 136/248 [00:00<00:00, 203.22 examples/s][A
Postprocessing dataset (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 160/248 [00:00<00:00, 210.97 examples/s][A
Postprocessing dataset (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 184/248 [00:01<00:00, 210.83 examples/s][A
Postprocessing dataset (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 206/248 [00:01<00:00, 203.61 examples/s][A
Postprocessing dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 234/248 [00:01<00:00, 219.32 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [00:01<00:00, 182.00 examples/s]
01/29/2025 06:55:31 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/3_Linh/parquet_files/3_Linh_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:00<00:00, 15.08s/it]
01/29/2025 06:55:35 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 242 examples [00:00, 80807.39 examples/s]
01/29/2025 06:55:35 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.89s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.66s/it]
Preparing prompts (num_proc=32):   0%|          | 0/242 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/242 [00:00<00:05, 39.12 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 32/242 [00:00<00:01, 107.30 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 56/242 [00:00<00:01, 149.36 examples/s]Preparing prompts (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 88/242 [00:00<00:00, 196.64 examples/s]Preparing prompts (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 112/242 [00:00<00:00, 206.63 examples/s]Preparing prompts (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 136/242 [00:00<00:00, 159.09 examples/s]Preparing prompts (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 165/242 [00:01<00:00, 174.64 examples/s]Preparing prompts (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 186/242 [00:01<00:00, 181.41 examples/s]Preparing prompts (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 228/242 [00:01<00:00, 226.82 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 242/242 [00:01<00:00, 180.67 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:14<01:38, 14.11s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:29<01:30, 15.00s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:42<01:10, 14.10s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:55<00:53, 13.50s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:10<00:42, 14.21s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:24<00:27, 13.90s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:37<00:13, 13.66s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:46<00:00, 12.28s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/242 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/242 [00:00<00:07, 32.38 examples/s][A
Postprocessing dataset (num_proc=32):  10%|â–‰         | 24/242 [00:00<00:02, 77.12 examples/s][A
Postprocessing dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 56/242 [00:00<00:01, 156.71 examples/s][A
Postprocessing dataset (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 80/242 [00:00<00:01, 151.50 examples/s][A
Postprocessing dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 112/242 [00:00<00:00, 172.50 examples/s][A
Postprocessing dataset (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 144/242 [00:00<00:00, 201.09 examples/s][A
Postprocessing dataset (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 172/242 [00:01<00:00, 198.93 examples/s][A
Postprocessing dataset (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 193/242 [00:01<00:00, 196.46 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 214/242 [00:01<00:00, 187.51 examples/s][A
Postprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 242/242 [00:01<00:00, 208.00 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 242/242 [00:01<00:00, 166.49 examples/s]
01/29/2025 06:57:35 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/11_Há»“ng/parquet_files/11_Há»“ng_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:48<00:00, 13.60s/it]
01/29/2025 06:57:39 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 300 examples [00:00, 94232.85 examples/s]
01/29/2025 06:57:39 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.91s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.97s/it]
Preparing prompts (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 48.73 examples/s]Preparing prompts (num_proc=32):  10%|â–ˆ         | 30/300 [00:00<00:02, 110.12 examples/s]Preparing prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:00<00:01, 200.26 examples/s]Preparing prompts (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [00:00<00:00, 219.18 examples/s]Preparing prompts (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 129/300 [00:00<00:00, 216.30 examples/s]Preparing prompts (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 156/300 [00:00<00:00, 216.18 examples/s]Preparing prompts (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 183/300 [00:00<00:00, 214.32 examples/s]Preparing prompts (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 219/300 [00:01<00:00, 238.87 examples/s]Preparing prompts (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [00:01<00:00, 239.09 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 223.01 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/10 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  10%|â–ˆ         | 1/10 [00:16<02:25, 16.20s/it] ... :  20%|â–ˆâ–ˆ        | 2/10 [00:29<01:55, 14.45s/it] ... :  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:42<01:37, 13.89s/it] ... :  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:57<01:25, 14.26s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:08<01:06, 13.21s/it] ... :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:20<00:50, 12.63s/it] ... :  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:37<00:42, 14.02s/it] ... :  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:51<00:28, 14.09s/it] ... :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:03<00:13, 13.41s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:09<00:00, 11.29s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 10/300 [00:00<00:05, 50.00 examples/s][A
Postprocessing dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/300 [00:00<00:01, 145.18 examples/s][A
Postprocessing dataset (num_proc=32):  20%|â–ˆâ–ˆ        | 60/300 [00:00<00:01, 152.72 examples/s][A
Postprocessing dataset (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [00:00<00:00, 228.62 examples/s][A
Postprocessing dataset (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 129/300 [00:00<00:00, 216.48 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [00:00<00:00, 265.01 examples/s][A
Postprocessing dataset (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [00:00<00:00, 237.92 examples/s][A
Postprocessing dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [00:01<00:00, 236.98 examples/s][A
Postprocessing dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [00:01<00:00, 239.82 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 217.64 examples/s]
01/29/2025 07:00:03 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/44_Má»¹/parquet_files/44_Má»¹_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:12<00:00, 13.20s/it]
01/29/2025 07:00:08 - INFO - __main__ - *** Load annotated dataset ***
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 249 examples [00:00, 38599.32 examples/s]
01/29/2025 07:00:08 - INFO - __main__ - *** Load pretrained model ***
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.35s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.25s/it]
Preparing prompts (num_proc=32):   0%|          | 0/249 [00:00<?, ? examples/s]Preparing prompts (num_proc=32):   3%|â–Ž         | 8/249 [00:00<00:06, 37.64 examples/s]Preparing prompts (num_proc=32):  13%|â–ˆâ–Ž        | 32/249 [00:00<00:01, 111.02 examples/s]Preparing prompts (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 72/249 [00:00<00:00, 185.01 examples/s]Preparing prompts (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 96/249 [00:00<00:00, 187.76 examples/s]Preparing prompts (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 120/249 [00:00<00:00, 191.37 examples/s]Preparing prompts (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 144/249 [00:00<00:00, 189.37 examples/s]Preparing prompts (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 168/249 [00:01<00:00, 172.58 examples/s]Preparing prompts (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 192/249 [00:01<00:00, 181.18 examples/s]Preparing prompts (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 235/249 [00:01<00:00, 229.95 examples/s]Preparing prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:01<00:00, 185.13 examples/s]
/home/pc/anaconda3/envs/dataspeech/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
 ... :   0%|          | 0/8 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... :  12%|â–ˆâ–Ž        | 1/8 [00:13<01:36, 13.72s/it] ... :  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:28<01:24, 14.07s/it] ... :  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:43<01:12, 14.60s/it] ... :  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:56<00:57, 14.26s/it] ... :  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [01:08<00:40, 13.34s/it] ... :  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:20<00:25, 12.98s/it] ... :  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:34<00:12, 12.99s/it] ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:44<00:00, 12.16s/it]
Postprocessing dataset (num_proc=32):   0%|          | 0/249 [00:00<?, ? examples/s][A
Postprocessing dataset (num_proc=32):   3%|â–Ž         | 8/249 [00:00<00:07, 33.72 examples/s][A
Postprocessing dataset (num_proc=32):  16%|â–ˆâ–Œ        | 40/249 [00:00<00:01, 125.07 examples/s][A
Postprocessing dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 64/249 [00:00<00:01, 142.75 examples/s][A
Postprocessing dataset (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 96/249 [00:00<00:00, 186.70 examples/s][A
Postprocessing dataset (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 120/249 [00:00<00:00, 198.39 examples/s][A
Postprocessing dataset (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 144/249 [00:00<00:00, 199.04 examples/s][A
Postprocessing dataset (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 176/249 [00:01<00:00, 189.13 examples/s][A
Postprocessing dataset (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 214/249 [00:01<00:00, 221.14 examples/s][A
Postprocessing dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 242/249 [00:01<00:00, 221.51 examples/s][APostprocessing dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:01<00:00, 180.28 examples/s]
01/29/2025 07:02:07 - INFO - __main__ - Saved train split to Parquet: processed3_parquet/12_Tháº¯ng/parquet_files/12_Tháº¯ng_train.parquet
 ... : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:46<00:00, 13.32s/it]
Running for speaker: Diá»‡p (file: 41_Diá»‡p.parquet)
Running for speaker: Oanh (file: 46_Oanh.parquet)
Running for speaker: Huá»‡ (file: 25_Huá»‡.parquet)
Running for speaker: Thá»§y (file: 34_Thá»§y.parquet)
Running for speaker: HÃ¹ng (file: 32_HÃ¹ng.parquet)
Running for speaker: ThÃ¡i (file: 9_ThÃ¡i.parquet)
Running for speaker: Chiáº¿n (file: 29_Chiáº¿n.parquet)
Running for speaker: LÃ¢m (file: 39_LÃ¢m.parquet)
Running for speaker: Háº¡ (file: 40_Háº¡.parquet)
Running for speaker: My (file: 6_My.parquet)
Running for speaker: Quá»³nh (file: 13_Quá»³nh.parquet)
Running for speaker: KiÃªn (file: 8_KiÃªn.parquet)
Running for speaker: VÅ© (file: 20_VÅ©.parquet)
Running for speaker: Miu (file: 31_Miu.parquet)
Running for speaker: Hiáº¿c (file: 36_Hiáº¿c.parquet)
Running for speaker: HÆ°ng (file: 30_HÆ°ng.parquet)
Running for speaker: Nguyá»‡t (file: 27_Nguyá»‡t.parquet)
Running for speaker: PhÆ°Æ¡ng (file: 38_PhÆ°Æ¡ng.parquet)
Running for speaker: Ly (file: 16_Ly.parquet)
Running for speaker: NhÆ° (file: 14_NhÆ°.parquet)
Running for speaker: Huy (file: 4_Huy.parquet)
Running for speaker: Vy (file: 2_Vy.parquet)
Running for speaker: Nhi (file: 45_Nhi.parquet)
Running for speaker: Huy (file: 21_Huy.parquet)
Running for speaker: NhÃ£ (file: 43_NhÃ£.parquet)
Running for speaker: HÆ°á»ng (file: 10_HÆ°á»ng.parquet)
Running for speaker: Háº£i (file: 26_Háº£i.parquet)
Running for speaker: Háº­u (file: 37_Háº­u.parquet)
Running for speaker: Minh (file: 7_Minh.parquet)
Running for speaker: Há»“ (file: 35_Há»“.parquet)
Running for speaker: Tuáº¥n (file: 23_Tuáº¥n.parquet)
Running for speaker: Hiá»n (file: 17_Hiá»n.parquet)
Running for speaker: Long (file: 24_Long.parquet)
Running for speaker: Lan (file: 1_Lan.parquet)
Running for speaker: Quyáº¿t (file: 22_Quyáº¿t.parquet)
Running for speaker: Nhi (file: 5_Nhi.parquet)
Running for speaker: Nghá»‹ (file: 28_Nghá»‹.parquet)
Running for speaker: Äoan (file: 42_Äoan.parquet)
Running for speaker: Äá»©c (file: 15_Äá»©c.parquet)
Running for speaker: Nam (file: 19_Nam.parquet)
Running for speaker: Máº¡nh (file: 18_Máº¡nh.parquet)
Running for speaker: HoÃ ng (file: 33_HoÃ ng.parquet)
Running for speaker: Linh (file: 3_Linh.parquet)
Running for speaker: Há»“ng (file: 11_Há»“ng.parquet)
Running for speaker: Má»¹ (file: 44_Má»¹.parquet)
Running for speaker: Tháº¯ng (file: 12_Tháº¯ng.parquet)
Processing completed for all files.
